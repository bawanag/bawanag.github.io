<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><title>Comparison of Different Deep Learning technologies toward Choosing Appropriate Convolution Neural Network architecture</title><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><meta http-equiv="X-UA-Compatible" content="IE=Edge，chrome=1"><meta name="description" content=""><meta name="keywords"><meta name="author" content="Yuanjian Tao"><link rel="short icon" href="/images/favicon.png"><link rel="icon" href="/images/favicon.png"><!--[if lt IE 9]>
<script src="/js/modernizr.js"></script>
<![endif]-->
<link rel="stylesheet" href="/css/iconfont.css">

<link rel="stylesheet" href="/css/index.css?v=202101252249.css">
<link rel="stylesheet" href="/css/info.css?v=202101252249.css">
<link href="https://cdn.bootcss.com/highlight.js/9.15.9/styles/github.min.css" rel="stylesheet"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
</head><body><header id="header" class="header-navigation"><nav><div class="logo"><a href="http://example.com">Blog</a></div><h2 id="mnavh"><span class="navicon"></span></h2><ul id="starlist"><li><a href="/">首页</a></li><li><a href="/time.html">时间轴</a></li></ul><div class="searchbox"><div id="search_bar" class="search_bar"><input id="keyboard" placeholder="想搜点什么呢.." type="text" name="keyboard" autocomplete="off" class="input"><p class="search_ico"><span></span></p></div></div></nav></header><article><main><div class="con_warp"><div class="infosbox"><div class="newsview"><h3 class="news_title">Comparison of Different Deep Learning technologies toward Choosing Appropriate Convolution Neural Network architecture</h3><div class="bloginfo"><ul><li class="author">作者：<a href="/">Yuanjian Tao</a></li><li class="lmname"><a href="/"></a></li><li class="timer">时间：2020-12-21 02:41:20</li><li class="view"><span id="busuanzi_value_page_pv">99</span><span>次访问</span></li></ul></div><div class="tags"></div><div class="news_con"><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>In this project, in order to give guidance before inference CNN models in the<br>different computing environments, I investigate part of popular state-of-the-art<br>CNN model research and source code to find out their similarities and<br>differences. Also, the compression technologies and models are evaluated by the<br>benchmark after model compression.</p>
<p>Though investigating and analysing the benchmark of every model and compression<br>technologies, I find that using dedicated accelerators can significantly<br>increase model efficiency when using more complex models. When the hardware and<br>power support are limited, model compression technologies and compressed models<br>can significantly elevate efficiency. However, after analysing the pruning model<br>results of this project, I found that although pruning can significantly<br>decrease the inference and running time, it may increase the model size and<br>memory footprint unless the pruning rate is higher than a threshold. However,<br>when the pruning rate is too high, the model performance could reduce<br>significantly. About int8 quantization, it can significantly reduce the model<br>size and running time when processing multiple tasks. But only the models which<br>introduce residual learning(<a href="#_ENREF_15">He et al., 2016</a>) can gain the<br>inference time decrease. I also try to use two or more compression technologies<br>in the same model, but I found that although different compression methods can<br>implement the same models and gain further size and memory footprint deduction,<br>it cannot bring comparable performance and gain higher efficiency.</p>
<p>Based on the all the results of this project, there is no model with the<br>absolute performance and efficiency advantage. So in order to choose the best<br>models in a specific computing environment, trade-off between different model<br>compression technologies is very important.</p>
<p><strong>List of Abbreviations</strong></p>
<table>
<thead>
<tr>
<th>EDLS</th>
<th>Embedded Deep Learning System</th>
</tr>
</thead>
<tbody><tr>
<td>CNNs</td>
<td>Convolutional neural networks</td>
</tr>
<tr>
<td>DNNs</td>
<td>Deep Neural Networks</td>
</tr>
<tr>
<td>GConv</td>
<td>Group Convolution</td>
</tr>
<tr>
<td>SWaP</td>
<td>low size, weight, and power</td>
</tr>
<tr>
<td>FPGA</td>
<td>Field-programmable gate array</td>
</tr>
<tr>
<td>ILSVRC2012</td>
<td>ImageNet Large Scale Visual Recognition Challenge 2012</td>
</tr>
<tr>
<td>GFLOPs</td>
<td>Giga Floating-point Operations Per Second</td>
</tr>
<tr>
<td>CUDA</td>
<td>Compute Unified Device Architecture</td>
</tr>
<tr>
<td>BN</td>
<td>Batch Normalizing</td>
</tr>
<tr>
<td>ReLU</td>
<td>Rectified Linear Unit</td>
</tr>
<tr>
<td>AM</td>
<td>Arithmetic Mean</td>
</tr>
<tr>
<td>GM</td>
<td>Geometric Mean</td>
</tr>
<tr>
<td>ARC3</td>
<td>Advanced Research Computing3</td>
</tr>
<tr>
<td>ARC4</td>
<td>Advanced Research Computing4</td>
</tr>
</tbody></table>
<p>Table of Contents</p>
<blockquote>
<p>  <a href="#_Toc49462111">Summary    iii</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462112">Acknowledgements    iv</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462113">List of Abbreviations    v</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462114">Table of Contents    vi</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462115">List of Figures    1</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462116">List of Table    3</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462117">Chapter 1 Introduction    4</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462118">1.1 Project Aim    4</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462119">1.2 Objectives    4</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462120">1.3 Deliverables    4</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462121">1.4 Ethical    5</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462122">Chapter 2 Background research    6</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462123">2.1 Literature Survey    6</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462124">2.1.1 Convolution Neural Networks(CNNs)    6</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462125">2.1.2 Inference cost in Embedded Deep Learning<br>  System(EDLS)    7</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462126">2.1.3 Pruning    8</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462127">2.1.4 Quantization    9</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462128">2.1.5 MobileNet    9</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462129">2.1.6 ShuffleNet    12</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462130">2.1.7 Conclusion    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462131">2.2 Methods and Techniques    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462132">2.2.1 Deep Learning Framework    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462133">2.2.2 Pruning    15</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462134">2.2.3 Quantization    15</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462135">2.2.4 CNNs pre-trained models    16</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462136">2.2.5 Testing Dataset    16</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462137">2.3 Choice of methods    17</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462138">Chapter 3 Datasets and Experimental Design    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462139">3.1 Datasets/Data Sources    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462140">3.2 Experimental Design    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462141">3.2.1 Test data collection and allocation    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462142">3.2.2 Prepare trained, pruned, and quantized models    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462143">3.2.3 Test design    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462144">3.2.4 Data analysis    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462145">Chapter 4 Results of the Empirical Investigation    22</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462146">4.1 Pre-trained float32 models comparison    22</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462147">4.2 Pruned models    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462148">4.3 Quantized models    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462149">4.4 Mixed comparison of pruning and quantization    34</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462150">4.5 Horizontal comparison of model performance and efficiency under<br>  hardware constraints    38</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462151">Chapter 5 Validation of Results    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462152">5.1 Testbed    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462153">5.2 Data collection and analysis    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462154">5.3 Compare results with other same researches    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462155">5.4 Conclusion    43</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462156">Chapter 6 Conclusions and Future Work    44</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462157">6.1 Conclusions    44</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462158">6.2 Future Work    44</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462159">List of References    45</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462160">Appendix A External Materials    48</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462161">A.1  All model performance    48</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462162">A.2  All model resource and time consumption    49</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462163">A.3  Project Git repository link    50</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462164">Appendix B Ethical Issues Addressed    51</a></p>
</blockquote>
<p>List of Figures</p>
<blockquote>
<p>  <a href="#_Toc49462165">Figure 2.1 Performance vs. power scatter plot of publicly announced AI<br>  accelerators and processors(Reuther et al., 2019)    8</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462166">Figure 2.2 Pruning process    8</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462167">Figure 2.3 Standard and Depthwise Separable convolution with Batch<br>  Normalizing(BN) Transform and ReLU.    10</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462168">Figure 2.4 MobileNetv2 Standard Convolution with BN, stride = 1 and stride<br>  = 2 Convolutional blocks    11</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462169">Figure 2.5 Channel shuffle layer.    12</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462170">Figure 2.6 ShuffleNetv1 residual convolution, stride = 1 GConv, and stride<br>  = 2 GConv blocks.    13</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462171">Figure 2.7  ShuffleNetv2 basic block and spatial down sampling<br>  block.    13</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462172">Figure 2.8 Equation of BN(Liu et al., 2017)    15</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462173">Figure 2.9 CIFAR-10 image sample    16</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462174">Figure 2.10 ImageNet Large Scale Visual Recognition Challenge<br>  2012(ILSVRC2012) validset class07693725 00044186 image<br>  sample    17</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462175">Figure 3.1 quantized model dataflow.    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462176">Figure 4.1 The CUDA memory footprint after float32 pre-trained models are<br>  loaded (a), the models’ memory footprint when they implement by one ARC3 CPU<br>  computing node(b), the models estimated GFLOPs (c), practical storage usage<br>  (d), and The models’ parameter numbers(e)    23</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462177">Figure 4.2 V100 GPU test all the ImageNet validset, model inference<br>  time(a), and The total time consumption of classifying 5k ImageNet testset<br>  full resolution images(b); Using ARC3 CPU node to test the float32<br>  pre-trained models, model inference time(c) and The total time consumption<br>  of classifying 5k ImageNet testset full resolution<br>  images(d)    24</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462178">Figure 4.3 Each model Top-1 accuracy(a), Top-5 accuracy(b), model precision<br>  and recall(c), and F1-score(d)    25</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462179">Figure 4.4 The CUDA memory usage after pre-pruned and fine-tuned models are<br>  loaded (a), the models estimated GFLOPs (b), practical storage usage (c),<br>  and The models’ parameter numbers(d)    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462180">Figure 4.5  Compare different ResNet50 pruning rate and original<br>  pre-trained ResNet50 Top-1 accuracy(a), Top-5 accuracy(b), precision,<br>  recall(c), and F1-score(d).    27</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462181">Figure 4.6 Pruning model inference time(a) and classify 5k full resolution<br>  images time(b)    28</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462182">Figure 4.7 Using one ARC3 normal CPU node to test the host memory usage(a),<br>  model inference time(c), model running time(e), and their change(b), (d),<br>  and (f).    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462183">Figure 4.8 upper two figures: quantized models host memory footprint(a) and<br>  compared with float pre-trained models the reduce percentage of host memory<br>  footprint. Lower two figures: quantization models storage space usage(c) and<br>  using storage reduce rate(d).    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462184">Figure 4.9 upper two figures: quantization models inference time(a) and<br>  compared with float32 pre-trained models inference time change(b). lower two<br>  figures: each quantization models running five thousand ImageNet testset<br>  full resolution image time consumption(c) and compared with float32<br>  pre-trained model running time change(d)    31</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462185">Figure 4.10 Compared with float32, quantization models Top-1 accuracy(a)<br>  and change(b); Top-5 accuracy(c) and change(d)    32</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462186">Figure 4.11 Compared with float32, quantized models F1-score(a) and reduce<br>  range(b); Recall/precision(c) and recall(d)/precision(e)<br>  change.    33</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462187">Figure 4.12 upper two figures:  pre-trained, pruned and quantized ResNet50<br>  models host memory footprint(a) and compared with float32 pre-trained<br>  ResNet50 models the reduced percentage of host memory footprint. Lower two<br>  figures: models storage space usage(c) and compared with float32 pre-trained<br>  ResNet50 models using storage reduce rate(d).    34</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462188">Figure 4.13 upper two figures: pre-trained, pruned, and quantized ResNet50<br>  models inference time(a) and compared with float32 pre-trained ResNet50<br>  inference time change(b). lower two figures: each ResNet50 models running<br>  five thousand ImageNet testset full resolution image time consumption(c) and<br>  compared with float32 pre-trained ResNet50 running time<br>  change(d)    35</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462189">Figure 4.14  models Top-1 accuracy(a), Top-5 accuracy(c),  and their<br>  reduction rate(b)(d)    36</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462190">Figure 4.15 Compared with ResNet50_float32, F1-score(a), Precision/Recall<br>  (c), and their reduction rate(b)(d)(e).    37</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462191">Figure 5.1 ARC4 Sun Grid Engine (SGE) queue    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462192">Figure 5.2 Log testbed information in each test.    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462193">Figure 5.3 Compared the memory footprint statistic between this project(a)<br>  and Muhammed et al. (Muhammed et al., 2017)    42</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462194">Figure 5.4 ResNet50 and quantized pruned ResNet50 inference time(a),<br>  inference time change(b), and quantization + pruning VGG16/ResNet50<br>  inference time change(c) from the research of Qin et al. (Qin et al.,<br>  2018)    43</a></p>
</blockquote>
<p>List of Table</p>
<blockquote>
<p>  <a href="#_Toc49462195">Table 2.1 Compare different CNN size and implement cost    7</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462196">Table 2.2 MobileNetv1 network layer structure and<br>  parameters    10</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462197">Table 2.3 MobileNetv2 Body Architecture    11</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462198">Table 2.4 Compare MobileNet v1 and v2. (figures in MobileNet v1 Top-1 are<br>  cited from Sandler (Sandler et al., 2018))    11</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462199">Table 4.1 the models with the best performance among different<br>  properties    38</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462200">Table 5.1 Compared the model performance results between this project and<br>  Pytorch Model zoo(Facebook).    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc49462201">Table 5.2  Compared the model performance results between this project and<br>  research from Li et al. (Lin et al., 2020)    42</a></p>
</blockquote>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><br>Introduction</h1><h2 id="Project-Aim"><a href="#Project-Aim" class="headerlink" title="Project Aim"></a>Project Aim</h2><p>Convolution Neural Networks(CNNs) have been used in different fields in recent<br>years. Benefiting from performance elevation, the general devices such as<br>smartphones and CCTV video recorders also contain more intelligent performance<br>when using CNNs. However, with the performance improvement of CNNs, the required<br>computing power also increases. Restricted by limited power and the size of<br>devices, the majority of devices cannot reach the computing power demand of<br>state-of-the-art CNNs. Therefore, different compression technologies and models<br>are proposed. So it is very important to review them before choosing and using a<br>CNN model.</p>
<p>In this project, different cutting edge CNN models and compression technologies<br>will be compared in vertical and horizontal angles. Meanwhile, different<br>properties and benchmarks of models will be evaluated to extract the crucial<br>characteristics. The project results should provide accurate model performance<br>and efficiency. Also, different compression technologies are analyzed and then<br>present suggestions when using them.</p>
<h2 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h2><p>This project will meet the objectives and obtain the results are following:</p>
<ol>
<li><p>Understanding the state-of-the-art CNNs and compressed models structure. And<br> comparing their similarities and differences.</p>
</li>
<li><p>Acquiring the models which are worthy of study. Using model compression<br> technologies such as pruning and quantization to compress CNN models.<br> Logging and comparing the basic differentiation between compressed and<br> uncompressed models.</p>
</li>
<li><p>Using different hardware and evaluation methods to benchmark all the<br> generated models and noting all the useful data in the database.</p>
</li>
<li><p>Processing the raw data and evaluating all the models performance and<br> efficiency. Giving detailed and accurate test results.</p>
</li>
<li><p>Based on benchmark and Investigation results, giving my comments and<br> recommendation when using different CNN architecture and compression<br> technologies.</p>
</li>
</ol>
<h1 id="Background-research"><a href="#Background-research" class="headerlink" title="Background research"></a><br>Background research</h1><h2 id="Literature-Survey"><a href="#Literature-Survey" class="headerlink" title="Literature Survey"></a>Literature Survey</h2><p>This chapter discusses the research and technology about CNNs and compressed<br>technologies of CNNs. It explains why CNNs need to be compressed and previous<br>research about how to compress CNNs.</p>
<p>Along with the development of CNNs, in order to achieve higher performance, CNN<br>models become larger which demands more calculational electrical power.<br>Meanwhile, they own better performance which attracts various devices,<br>especially embedded and mobile devices to use them. However, state-of-the-art<br>DNNs often are trained in a distributed computing environment with dedicated<br>chip which can provide significant computational power. Also, the models occupy<br>more storage and cannot easily run on a limited power platform. Hence, a<br>trade-off between model preference and efficiency is very necessary in<br>particular cases.</p>
<p>In order to solve these issues, many algorithms and fine-tuning methods are<br>emerging such as pruning, quantization, and compressed models.</p>
<h3 id="Convolution-Neural-Networks-CNNs"><a href="#Convolution-Neural-Networks-CNNs" class="headerlink" title="Convolution Neural Networks(CNNs)"></a>Convolution Neural Networks(CNNs)</h3><p>Along with CNNs development, models become larger which brings better accuracy.<br>Meanwhile, higher computing power and hardware demand also increase CNNs<br>threshold.</p>
<p>In 1968, Hubel and Wiesel found that brain context has a very spatial<br>arrangement that can find the objective features and classify them. (<a href="#_ENREF_18">Hubel and<br>Wiesel, 1968</a>) Based on their theory, many researchers try to<br>simulate this behavior which calls convolution operation. In 1998, LeCun(<a href="#_ENREF_23">LeCun<br>et al., 1998</a>) successfully simulated this phenomenon and construct<br>first CNN which had significant performance when implementing it in the image<br>classification. Because of hardware and computing power restriction, in this<br>research, LeCun <em>et al.</em> implemented a 7 layer network which used 2 convolution<br>layers, 2 maxpooling, 2 fully connection, and one Gaussian connection. Along<br>with hardware performance, algorithm, and computing power increases, in 2014,<br>AlexNet was introduced which could be trained by muti-GPU. Meanwhile, it added<br>dropout, data augmentation, and local response normalization.(<a href="#_ENREF_21">Krizhevsky,<br>2014</a>) In the same year, VGG(<a href="#_ENREF_41">Simonyan and Zisserman,<br>2014</a>) had deeper layers and smaller convolutional kernels which<br>proved that raising the network depth can significantly enhance accuracy. In<br>GoogleNet, inception architecture uses optimal local sparse structure in a<br>convolutional vision network. (<a href="#_ENREF_43">Szegedy et al., 2015</a>) In order to<br>reduce degradation and train deeper models in DNN, residual learning was<br>introduced which can significantly raise the network layers and help to increase<br>the accuracy. (<a href="#_ENREF_15">He et al., 2016</a>) In the research of Xie et al.,<br>residual learning and inception theories are mixed and used in ResNeXt although<br>it cannot offer very good performance. (<a href="#_ENREF_45">Xie et al., 2017</a>)</p>
<p>As the Table 2.1 illustrates that along with CNNs development, networks can be<br>built with deeper layers and consider more specific cases. Hence, the parameters<br>and size of CNNs become larger which need more training time and implemented<br>power.</p>
<blockquote>
<p>  Table 2.1 Compare different CNN size and implement cost</p>
</blockquote>
<table>
<thead>
<tr>
<th>Model Architecture</th>
<th>Year</th>
<th>Total size (MB)</th>
<th>Parameter number</th>
<th>Giga F<strong>l</strong>oating-point Operations Per Second(GFLOPs)</th>
</tr>
</thead>
<tbody><tr>
<td>LeNet</td>
<td>1998</td>
<td>25.68</td>
<td>5,612,426</td>
<td>0.04</td>
</tr>
<tr>
<td>AlexNet</td>
<td>2014</td>
<td>233.96</td>
<td>716,084,224</td>
<td>0.72</td>
</tr>
<tr>
<td>VGG11</td>
<td>2014</td>
<td>632.78</td>
<td>132,863,336</td>
<td>7.63</td>
</tr>
<tr>
<td>VGG11_bn</td>
<td>2014</td>
<td>506.85</td>
<td>132,868,840</td>
<td>7.65</td>
</tr>
<tr>
<td>VGG19</td>
<td>2014</td>
<td>787.31</td>
<td>143,667,240</td>
<td>19.67</td>
</tr>
<tr>
<td>VGG19_bn</td>
<td>2014</td>
<td>900.66</td>
<td>143,678,248</td>
<td>19.7</td>
</tr>
<tr>
<td>GoogleNet_inception</td>
<td>2015</td>
<td>144.43</td>
<td>13,004,888</td>
<td>1.52</td>
</tr>
<tr>
<td>ResNet18</td>
<td>2016</td>
<td>279.2</td>
<td>60,192,808</td>
<td>11.62</td>
</tr>
<tr>
<td>ResNet152</td>
<td>2016</td>
<td>279.2</td>
<td>60,192,808</td>
<td>11.62</td>
</tr>
<tr>
<td>ResNeXt50_32x4 dimension</td>
<td>2017</td>
<td>95.7</td>
<td>25,028,904</td>
<td>4.29</td>
</tr>
<tr>
<td>ResNeXt101_32x8dimension</td>
<td>2017</td>
<td>339</td>
<td>88,791,336</td>
<td>16.55</td>
</tr>
</tbody></table>
<h3 id="Inference-cost-in-Embedded-Deep-Learning-System-EDLS"><a href="#Inference-cost-in-Embedded-Deep-Learning-System-EDLS" class="headerlink" title="Inference cost in Embedded Deep Learning System(EDLS)"></a>Inference cost in Embedded Deep Learning System(EDLS)</h3><p>In the EDLS, there are heterogeneous processors and accelerators to implement<br>DNNs. They have different operational instruction sets, performance, and<br>application scenarios. However, in order to raise the training efficiency and<br>model performance, muti-chip assessors, or distributed systems often are<br>introduced in the training. There is a different performance gap between<br>training and running systems. Therefore, cutting the parameter number and<br>transforming models to adapt low size, weight, and power (SWaP) accelerators is<br>very necessary.</p>
<p>Reuther <em>et al.</em>,(<a href="#_ENREF_35">Reuther et al., 2019</a>) try to evaluate the<br>Machine Learning accelerators. (as Figure 2.1 shown) In this research, they<br>found that a majority of low-SWaP processors and accelerators only support<br>inference lower Integer computation precision. A very limited number of low-SWaP<br>devices and accelerators (such MIT Eyeriss and TPUEdge) can implement higher<br>computation precision. However, they cannot offer comparable performance and<br>reasonable power performance. In the embedded chips and systems set, although<br>they already increase the computation precision, they have almost the same Giga<br>operations per second (GOps/s) as very low power and research chips.</p>
<p>Compared with low-SWaP accelerators, data center chips and cards have high<br>computation precision and significant GOps/s in single-chip and card. In<br>state-of-the-art DNNs training, they can be implemented by distributed system or<br>service system which provides more performance in the training and generation.<br>For instance, according to Sanh <em>et al.</em>, model parameter number have rapid<br>growth which brings significant improvement in Natural Language Processing<br>(NLP). (<a href="#_ENREF_38">Sanh et al., 2019</a>) Such BERT-Large needs a DGX-2 server<br>with 16 V100 cards which need under 3 day to train its model. If using a<br>distributed 16 DGX-2H nodes system with V100 cards, its training time can be<br>shortened to 236 min. (<a href="#_ENREF_30">Narasimhan</a>) Also, inference the high<br>parameter model consumes significant hardware resources. For example, using<br>1080Ti, which has the same core as P100 needs 5.2 milliseconds to inference<br>BERT-Large one time(<a href="#_ENREF_46">Yang et al., 2020</a>). Therefore, training and<br>inference original high parameter number models demand high-performance<br>accelerators.</p>
<img src="/2020/12/20/Comparison-of-Different-Deep-Learning-technologies/bb5314c8a9140c9c20e247cfddaf14aa.png" class="">

<blockquote>
<p>  Figure 2.1 Performance vs. power scatter plot of publicly announced AI<br>  accelerators and processors(<a href="#_ENREF_35">Reuther et al., 2019</a>)</p>
</blockquote>
<p>In conclusion, different power support brings different architecture and<br>operational characteristics between high and low power accelerators. In order to<br>introduce DNNs in EDLS, transforming the lower computation precision parameter,<br>and cutting the parameter number is very necessary.</p>
<h3 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h3><p>In order to implement state-of-the-art models in the low-SWaP hardware, pruning<br>some weights and perceptrons to slim network is one of the research directions<br>which not only can maintain the performance of models but also decrease the<br>demand of hardware.</p>
<p>In the research of CNN architecture, Denil et al. found that some of perceptrons<br>and weights have low accesses which are redundancies and should be removed.<br>Another result is that some perceptrons and weights need to not be learned at<br>all. (<a href="#_ENREF_7">Denil et al., 2013</a>) Another research by Hu et al. shows that<br>after Rectified Linear Unit(ReLU), VGG-16 has a very high percentage of zero<br>activations of a neuron. (<a href="#_ENREF_17">Hu et al., 2016</a>) In the research of<br>Srinivas and Babu, CNNs contain some redundant neurons which cannot increase<br>accuracy meanwhile add unnecessary calculation, memory, and storage. (<a href="#_ENREF_42">Srinivas<br>and Babu, 2015</a>) Therefore, pruning such weights and perceptrons can<br>achieve the slim target of models. Research of Denton <em>et al.</em> tries to use<br>pruning, (as Figure 2.2 shows) that through compression and fine-tuning, all the<br>layers of performance are restored. (<a href="#_ENREF_8">Denton et al., 2014</a>) As a<br>result, we can use pruning to remove those weights and perceptrons which not<br>only can decrease meaningless calculation and memory footprint, but also can<br>increase CNNs performance in specific cases.</p>
<blockquote>
<p>  Figure 2.2 Pruning process</p>
</blockquote>
<p>In partial training, Haeffele and Vidal(<a href="#_ENREF_14">Haeffele and Vidal, 2017</a>)<br>have proven that when networks have large enough parameters, they may have few<br>or one global minimizer. This means that training large networks can be easier<br>than smaller networks. Therefore, compared with designing and training a small<br>network, training, pruning, and fine-tuning a larger network should have higher<br>efficiency in small network training.(<a href="#_ENREF_12">Frankle and Carbin, 2018</a>)</p>
<h3 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a>Quantization</h3><p>Quantization is another method that can achieve a balance between performance<br>and efficiency. According to Gong <em>et al.</em>, using weight or conducting product<br>quantization can significantly decrease model size. But it only loses<br>classification accuracy in specific CNNs.(<a href="#_ENREF_13">Gong et al., 2014</a>)</p>
<p>High performance CNNs often trained by distributed or multi-GPU computing<br>environment. Because it can accelerate the training efficiency. Also, the models<br>can be designed deeper and larger. However, this also make the margin between<br>training environment and implemented devices become bigger. According to the<br>research of Dundar, the performance of training environment is dozens of times<br>than mobile devices.(<a href="#_ENREF_10">Dundar et al., 2013</a>) Meanwhile, low price and<br>power consumption chips such as X86 and ARM CPU cannot have very high efficiency<br>when running float32 models. If the embedded system chooses to allocate enough<br>memory, storage, and dedicated calculational chip such as ASIC, FPGA, or GPU to<br>run and speed up CNNs, it could significantly increase budget and power<br>consumption.</p>
<p>Quantization neural networks(QNNs) is one of theory which try to solve this<br>issue. Its thought is convert the higher precision operators and weights to be<br>lower. Although it loses some precision, it can significant reduce models size,<br>memory footprint, and increase running efficiency. Meanwhile, quantized<br>operators can be easily converted to simple and pre-existing operators which can<br>be easier implemented in the X86 or ARM architecture chips. (<a href="#_ENREF_20">Jain et al.,<br>2020</a>) As a result, it can reduce hardware demand and is beneficial<br>for low-SWaP devices.</p>
<h3 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h3><p>Both pruning and quantization are based on the existing model architecture. They<br>also have some limitations and constraints in the model compression. Some<br>researchers have concluded that using dedicated algorithms or cutting the<br>network layers also can achieve the same objective. Therefore, some<br>state-of-the-art CNNs already consider preference and efficiency when they are<br>designed like MobileNet.</p>
<p>In 2017, MobileNet was proposed. Benefiting from Depthwise Separable Convolution<br>to replace traditional convolutional algorithms and Width Multiplier, it<br>achieves considerable efficiency and usable accuracy. (<a href="#_ENREF_16">Howard et al.,<br>2017</a>)</p>
<p>In the MobileNetv1, the architecture of MobileNet is a direct connection from<br>input to output. Its architecture is similar to VGG. In convolutional<br>calculation, MobileNet is separated into two steps which are 3x3 depthwise<br>convolution and 1x1 convolution. (Figure 2.3 and Table 2.2) Because depthwise<br>convolution does not combine all the filters output to generate new feature<br>maps. Computational computation of Depthwise Convolution has 8 to 9 times less<br>than traditional convolution. Beneficial from width multiplierα, MobileNet can<br>adjust output channel and the parameter number which can reduce running time and<br>model size.</p>
<blockquote>
<p>  Figure 2.3 Standard and Depthwise Separable convolution with Batch<br>  Normalizing(BN) Transform and ReLU.</p>
</blockquote>
<blockquote>
<p>  Table 2.2 MobileNetv1 network layer structure and parameters</p>
</blockquote>
<table>
<thead>
<tr>
<th>Block type</th>
<th>Stride</th>
<th>First convolution kernel size</th>
<th>Input channel</th>
<th>Output channel</th>
</tr>
</thead>
<tbody><tr>
<td>Standard Convolution with BN</td>
<td>2</td>
<td>3x3</td>
<td>3</td>
<td>32</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>32</td>
<td>64</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>2</td>
<td>3x3</td>
<td>64</td>
<td>128</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>128</td>
<td>128</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>2</td>
<td>3x3</td>
<td>128</td>
<td>256</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>256</td>
<td>256</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>2</td>
<td>3x3</td>
<td>256</td>
<td>512</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>512</td>
<td>512</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>512</td>
<td>512</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>512</td>
<td>512</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>512</td>
<td>512</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>512</td>
<td>512</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>2</td>
<td>3x3</td>
<td>512</td>
<td>1024</td>
</tr>
<tr>
<td>Depthwise Separable Convolution</td>
<td>1</td>
<td>3x3</td>
<td>1024</td>
<td>1024</td>
</tr>
<tr>
<td>Average Pool</td>
<td>0</td>
<td>kernel size = 7x7</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fully connective</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Softmax</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>MobileNetv2 (<a href="#_ENREF_37">Sandler et al., 2018</a>) not only continually inherits<br>MobileNetv1 successful characteristic, but it introduces new methods such as<br>linear bottlenecks and inverted residuals which is similar to ResNet<br>architecture. (See Table 2.3 and Table 2.3) They further increase model accuracy<br>and further decrease model size and hardware demand(As Table 2.4).</p>
<blockquote>
<p>  Figure 2.4 MobileNetv2 Standard Convolution with BN, stride = 1 and stride =<br>  2 Convolutional blocks</p>
</blockquote>
<blockquote>
<p>  Table 2.3 MobileNetv2 Body Architecture</p>
</blockquote>
<table>
<thead>
<tr>
<th>Block type</th>
<th>Stride</th>
<th>Input channel</th>
<th>Output channel</th>
<th>expansion factor (t)</th>
<th>repeat number (n)</th>
<th>other</th>
</tr>
</thead>
<tbody><tr>
<td>Standard Convolution with BN</td>
<td>2</td>
<td>3</td>
<td>32</td>
<td></td>
<td></td>
<td>Convolution kernel size = 3x3</td>
</tr>
<tr>
<td>Residual block</td>
<td>1</td>
<td>32</td>
<td>16</td>
<td>6</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Residual block</td>
<td>2</td>
<td>16</td>
<td>24</td>
<td>6</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td>Residual block</td>
<td>2</td>
<td>24</td>
<td>32</td>
<td>6</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>Residual block</td>
<td>2</td>
<td>32</td>
<td>64</td>
<td>6</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td>Residual block</td>
<td>1</td>
<td>64</td>
<td>96</td>
<td>6</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>Residual block</td>
<td>2</td>
<td>96</td>
<td>160</td>
<td>6</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>Residual block</td>
<td>1</td>
<td>160</td>
<td>320</td>
<td>6</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Standard Convolution with BN</td>
<td>1</td>
<td>320</td>
<td>1280</td>
<td></td>
<td>1</td>
<td>Convolution kernel size = 1x1</td>
</tr>
<tr>
<td>Avgpool</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>kernel size = 7x7</td>
</tr>
<tr>
<td>Fully connective</td>
<td></td>
<td>1280</td>
<td>1000</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Softmax</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<blockquote>
<p>  Table 2.4 Compare MobileNet v1 and v2. (figures in MobileNet v1 Top-1 are<br>  cited from Sandler (<a href="#_ENREF_37">Sandler et al., 2018</a>))</p>
</blockquote>
<table>
<thead>
<tr>
<th>Model Architecture</th>
<th>Total size (MB)</th>
<th>Parameter number</th>
<th>GFLOPs</th>
<th>Top-1 accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>MobileNet v1</td>
<td>176.56</td>
<td>4,231,976</td>
<td>0.58</td>
<td>70.6%</td>
</tr>
<tr>
<td>MobileNet v2</td>
<td>166.81</td>
<td>3,504,872</td>
<td>0.31</td>
<td>71.9%</td>
</tr>
</tbody></table>
<h3 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h3><p>MobileNetv1 introduces some features about previous model theories which bring<br>significant promotion in performance per watt. Therefore, just as the ResNeXt,<br>ShuffleNetv1 tries to introduce more state-of-the-art architecture in order to<br>achieve better performance.</p>
<p>Compared with MobileNetv1, ShuffleNetv1(<a href="#_ENREF_47">Zhang et al., 2018</a>)<br>introduces similar ResNet architecture(<a href="#_ENREF_15">He et al., 2016</a>) as<br>earlier. Each ShuffleNetv1 also introduces 3x3 Depthwise separated convolution<br>instead of 3x3 convolution in each block. to sample the feature maps. Likewise,<br>Depthwise separated convolution also needs to use 1x1 convolution to sampling<br>the feature maps to lift or reduce dimensions of feature maps. But in the<br>ShuffleNetv1, 1x1 convolution is replaced by 1x1 group convolution(GConv).(See<br>Figure 2.6 middle)</p>
<p>Benefiting GConv, each input channels cannot be affected by another channels.<br>Although it can significantly reduce model parameters in GConv, it cannot take<br>another channel’s feature maps into account either so it could reduce the model<br>performance. Therefore, ShuffleNetv1 introduces channel shuffle layers(See<br>Figure 2.5) which can allocate channel pieces to each convolutional group.<br>(<a href="#_ENREF_47">Zhang et al., 2018</a>) It makes each output from each GConv consider<br>all the channels. Besides, it decreases calculational consumption.</p>
<p>After final convolution, in order to avoid residual is zero, ShuffleNetv1 also<br>needs to introduce the result from the previous block. Compared with ResNet(<a href="#_ENREF_15">He<br>et al., 2016</a>), it uses an average pooling and concats function to<br>sample and add all the cases to the final result. (See Figure 2.6 right)</p>
<blockquote>
<p>  Figure 2.5 Channel shuffle layer.</p>
</blockquote>
<blockquote>
<p>  Figure 2.6 ShuffleNetv1 residual convolution, stride = 1 GConv, and stride =<br>  2 GConv blocks.</p>
</blockquote>
<blockquote>
<p>  Figure 2.7 ShuffleNetv2 basic block and spatial down sampling block.</p>
</blockquote>
<p>In ShuffleNetv2(<a href="#_ENREF_27">Ma et al., 2018</a>), GConv is abandoned and replaced<br>by channel split in down sampling block (As shown in Figure 2.7left) According<br>to Ma, it can increase double output channel as ShuffleNetv1. Its architecture<br>can upkeep the parameter number and elevate its accuracy.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>This chapter illustrates that CNNs derive from bionics which simulates the<br>cerebral cortex. Along with development of calculational power and algorithms,<br>CNNs have significant performance and reach some achievements in image<br>classification and segmentation. In order to gain higher accuracy and better<br>performance, CNNs become deeper. This brings more parameters and high<br>calculational demand. But in practice, such as automatic drive and real-time<br>image/video analysis, they have very limited hardware and need to trade-off the<br>performance and calculational consumption. Therefore, model compression<br>technologies are very important when implementing DNNs in EDLS. Each compression<br>technology has different characteristics. In order to design efficient EDLS, we<br>need to know behaviour to understand when using different compression<br>technologies. (<a href="#_ENREF_33">Qin et al., 2018</a>) It can help developers to<br>construct approximate deep learning systems in embedded devices.</p>
<h2 id="Methods-and-Techniques"><a href="#Methods-and-Techniques" class="headerlink" title="Methods and Techniques"></a>Methods and Techniques</h2><p>In order to test and analysis all the cutting edge models and compression<br>technologies, investigating different deep learning frameworks, implementable<br>CNNs, determining which pruning and quantization methods can be review, and<br>appropriate dataset are very import. In this section, some different methods and<br>technologies which contribute to this project are discussed and trade-off their<br>strength and drawbacks.</p>
<h3 id="Deep-Learning-Framework"><a href="#Deep-Learning-Framework" class="headerlink" title="Deep Learning Framework"></a>Deep Learning Framework</h3><p>In order to implement state-of-the-art CNNs and compressed technologies,<br>choosing a clear, tidy, and efficient deep learning framework is very important<br>and could significantly affect development efficiency and available resources.</p>
<p>TensorFlow is one of the early examples of architecture in Machine Learning.<br>Because it has been used for so long, it has more documents, third-part tools,<br>pre-trained models, and various open-source resources. It already is implemented<br>by variety of heterogeneous hardware systems such as Embedded and<br>high-performance computers. (<a href="#_ENREF_1">Abadi et al., 2016</a>) As a result, in<br>industrial manufacturing, it is very popular.</p>
<p>Pytorch(<a href="#_ENREF_31">Paszke et al., 2017</a>) is one of the popular frameworks<br>which based on Torch architecture and deeply utilizes Python programming style.<br>It not only has as efficient C++ core as TensorFlow but also it owns simplified<br>and clear code structure to run automatic differentiation and weight update.<br>According to Paszke, Pytorch is based on Torch and utilizes Python program style<br>which brings better usability and reduces learning time cost. Benefiting from<br>these advantages, it is accepted by scientific researchers and primary Machine<br>Learning beginners. Plus, Python also provides interoperability and<br>extensibility which can easily implement and transform other open-source<br>libraries like Numpy. Clear class structure also helps the users locate crucial<br>code blocks and increase the usability. Finally, muti-core and multi thread<br>support offer GPU adaptation and higher efficiency. (<a href="#_ENREF_32">Paszke et al.,<br>2019</a>) Such Machine Learning framework could significantly increase<br>the efficiency of development; so it is a very powerful tool in scientific<br>research and education.</p>
<p>Because the majority of Deep Learning Framework can support various programming<br>languages, the programming language selection is very important. The development<br>of programming languages is accompanied by the development of computers. Early<br>languages ​​such as assembly and C have good operating efficiency. Along with<br>computer architecture development, programming languages not only have<br>acceptable efficiency but also should be easier to read and improve team<br>programming efficiency. Object-oriented programming languages ​​such as C++ and<br>Java have come out.</p>
<p>After the millennium, programming languages are used in different aspects which<br>demand language that should be easier to learn and more similar to natural<br>language. Therefore, Python and GO have become popular.</p>
<p>Benefit from rich API support, although TensorFlow is based on C++, it can be<br>dispatched by other programming languages such as Python, Java, and GO.(<a href="#_ENREF_2">Abadi<br>et al., 2017</a>) Compared with it, because Pytorch is based on Python<br>programming style, it only can support the target language Python and its<br>underlying language C++.</p>
<h3 id="Pruning-1"><a href="#Pruning-1" class="headerlink" title="Pruning"></a>Pruning</h3><p>In CNNs pruning, the most general method is to continually prune the network and<br>fine-tune. So which weights and perceptrons should be pruned and, need to be<br>pruned is a crucial problem.</p>
<p>Several researches use different methods to choose and cut the weight and<br>perceptrons in pruning architecture. To begin with, Lin et al. introduce L1-norm<br>based channel pruning. (<a href="#_ENREF_24">Li et al., 2016</a>) Its method though counts<br>each perceptron weights and sorts than to find which perceptron is useless.<br>Moreover, except for weights, the parameter of Batch Normalizing(BN) also can be<br>used to decide which perceptron should be cut. According to Liu et al., as<br>Figure 2.8 BN also needs to be trained in CNNs training γand βto limit the<br>output in the finite field. This means that if γis a scaling factor which<br>directory signs that which perceptrons are important.(<a href="#_ENREF_26">Liu et al.,<br>2017</a>) Therefore, through estimating BN parameters, the specific<br>perceptron can be decided whether prune it or not. Moreover, the feature map<br>also can be used to decide which perceptron should be pruned. In the research of<br>Hu et al., After ReLU Unit, much lower value output could become zero. (<a href="#_ENREF_17">Hu et<br>al., 2016</a>) So if some feature map contains too much zero output.<br>This means that the filters which generate their features is not very important.<br>Finally, in order to not lose the information when cutting the perceptrons and<br>weight. Lin et al. uses Filter Sketch to describe the important information in<br>the CNN and compare the original and pruned model covariance to reduce<br>information loss. (<a href="#_ENREF_25">Lin et al., 2020</a>) This can avoid time<br>consumption as another method to recursively prune and fine-tune the network.</p>
<table>
<thead>
<tr>
<th></th>
</tr>
</thead>
</table>
<blockquote>
<p>  Figure 2.8 Equation of BN(<a href="#_ENREF_26">Liu et al., 2017</a>)</p>
</blockquote>
<h3 id="Quantization-1"><a href="#Quantization-1" class="headerlink" title="Quantization"></a>Quantization</h3><p>In order to upkeep the performance of models and learning ability, quantization<br>also has many different methods to transform and fine-tune the model such as<br>dynamic quantization and static quantization. Quantization also can utilize<br>different storage types of weight such as int32, int16, int8, and int4.</p>
<p>In general, each pixel from images is saved by float32. Hence, CNNs architecture<br>also uses float32 to save the weigh. This architecture can reduce the logic<br>complexity of calculation and upkeep precision of the operators and weight. In<br>order to raise the calculation efficiency in x86 CPU, ARM, and specific<br>Field-programmable gate array(FPGA), quantization technology transforms the<br>weights storage type from float32 to the lower precision type like int8. But if<br>calculating float32 operators with int8, int8 has to raise the precision from<br>int8 to float32. This does not accelerate the calculation time. Therefore, the<br>image needs to reduce the precision from float32 to int8. (<a href="#_ENREF_19">Jacob et al.,<br>2018</a>) In partial, quantized models need to add quantize and<br>dequantize functions that wrap to pre-process float-valued input and output.</p>
<p>After the pre-processing, the model can be generated by dynamic quantization or<br>static quantization. In dynamic quantization, weights are saved by lower<br>precision type and initialize at random. The dataset and training method are the<br>same as the unquantized models. However, it cannot utilize GPU or other float<br>training accelerators to increase the training efficiency. As for static<br>quantization, a pre-trained model which is trained by float32 should be<br>provided. The quantization process needs to transform itself from float32 to a<br>lower precision type like int8 and ignore the precision loss. It also can be<br>continually fine-tuned in order to recover some accuracy.</p>
<h3 id="CNNs-pre-trained-models"><a href="#CNNs-pre-trained-models" class="headerlink" title="CNNs pre-trained models"></a>CNNs pre-trained models</h3><p>State-of-the-art CNNs often have enormous parameter numbers which demands large<br>amounts of computing power to train them. They also need fine-tuning to ensure<br>the best performance as possible. So if training all the models from random<br>weights, it could take a very long time. In order to decrease the time of<br>development and improve the rate of reuse code, TensorFlow and Pytorch community<br>also provide some pre-trained models which can be implemented straightforwardly.<br>But they are also trained by specific configuration so in the practical case,<br>they need some necessary modification and fine-tuning.</p>
<h3 id="Testing-Dataset"><a href="#Testing-Dataset" class="headerlink" title="Testing Dataset"></a>Testing Dataset</h3><p>Each CNN architecture model is trained on different datasets which could train<br>CNNs to get different functions. Therefore, the dataset also is a very important<br>aspect of CNNs.</p>
<p>In general, CIFAR-10 and CIFAR-100 dataset(<a href="#_ENREF_22">Krizhevsky and Hinton,<br>2009</a>) have 10 and 100 classifications, 60 thousand images, and<br>32x32 image resolution(as Figure 2.8 shown) which are often used in CNN models<br>test and benchmark. Because of lower data capacity, the majority of the Machine<br>Learning community does not provide pre-trained models. Besides, because of<br>lower image resolution and classification, it cannot be used in partial<br>applications.</p>
<img src="/2020/12/20/Comparison-of-Different-Deep-Learning-technologies/02e9e9c00c6f5a5869ffc56d44f48a76.png" class="" title="C:\\Users\\bawanag\\Desktop\\airplane10.png">

<blockquote>
<p>  Figure 2.9 CIFAR-10 image sample</p>
</blockquote>
<p>Another popular dataset is ImageNet dataset. (<a href="#_ENREF_36">Russakovsky et al.,<br>2015</a>) It utilizes full resolution images and now reaches up to 14<br>million images and 21 thousand synsets indexed. It not only can be used to<br>evaluate the model performance but also train models to use in practical<br>application. Benefiting from high resolution(as Figure 2.10 shown) and total<br>images number, models can learn more features and get more detail from images.<br>Therefore, ImageNet test results can be closer to actual test results. Also, it<br>can detect very subtle differences between models. But using it to train and<br>test the models could consume more time and computing power.</p>
<img src="/2020/12/20/Comparison-of-Different-Deep-Learning-technologies/0b75c96c7eac82317e6aad206dd5468b.jpeg" class="" title="E:\\坚果云同步\\学习\\artificial intelligence\\msc_project\\project\\pytorch_model_zoo_pretrain_model\\my_imagenet\\ImageNet_validset\\n07693725\\ILSVRC2012_val_00044186.JPEG">

<blockquote>
<p>  Figure 2.10 ImageNet Large Scale Visual Recognition Challenge<br>  2012(ILSVRC2012) validset class07693725 00044186 image sample</p>
</blockquote>
<h2 id="Choice-of-methods"><a href="#Choice-of-methods" class="headerlink" title="Choice of methods"></a>Choice of methods</h2><p>In section 2.2, many different tools and technologies are discussed that have<br>different characteristics. In order to use available resources to benchmark<br>different CNN architecture and compressed technologies, choosing appropriate and<br>avalanche technologies is very necessary. In this section, different<br>technologies will be compared and considered in specific cases to choose which<br>technologies are more suitable and efficient.</p>
<p>In this project, analysing model structure and extracting the log and<br>information is very important jobs. Pytorch can be qualified in this project.<br>Meanwhile, Pytorch also can be allocated in University of Leeds Advanced<br>Research Computing(ARC) clusters. ARC can provide state-of-the-art V100 GPU<br>which can significantly increase the testing efficiency and giving the<br>information of cutting edge hardware performance when running CNN. In order to<br>better use Pytorch, Python is chosen to be the programming language in this<br>project. Using Python also has many benefits. For example, its language style is<br>similar to native language. Also, it has rich documents of development can be<br>referred. This can be beneficial for this project.</p>
<p>Regarding pruning, based on the majority of pruning structure, pre-trained<br>models demand repeated pruning and fine-tuning to recover the accuracy of<br>networks. Because of the limited time and computing power of ARC3, pre-trained,<br>pre-pruned, and pre-fine-tuned models have to be chosen in this project. In the<br>research of Lin et al., (<a href="#_ENREF_25">Lin et al., 2020</a>) these models already<br>are provided in GitHub although the model architecture only can choose ResNet50.<br>But, they also provided different compressed rates which could be evaluated.</p>
<p>Quantization of CNNs often just needs to transform pre-trained models which save<br>the weight by float32 to lower precision operators. According to the Pytorch<br>online document,(<a href="#_ENREF_6">Contributors</a>) Pytorch only supports 8-bit weights<br>and activations in Neural Network, but it does not limit CNN architecture to do<br>quantization. Therefore, VGG, ResNet, inceptionv3, MobileNetv2, and ShuffleNetv2<br>will be quantized and tested in this project.</p>
<p>In Pytorch, TorchHub library(<a href="#_ENREF_11">Facebook</a>) can automatically implement<br>the different CNNs architecture structure. Also, it provides the pre-trained<br>parameter which can download pre-trained models straightforward which include<br>quantized and unquantized models. As a result, unquantized pre-trained models<br>will be downloaded in it. Its pre-trained models are transformed into quantized<br>models and tested as the benchmark.</p>
<p>About testing dataset, compared with CIFAR-10 and CIFAR-100, ImageNet can test<br>models more precisely. This can be beneficial for data analysis. Meanwhile,<br>Pytorch also has many pre-trained models that are trained by ILSVRC2012. This<br>can reduce the time consumption in training.</p>
<p>In conclusion, consider the trade-off between particular time, resource, and<br>technical limit. I will use Pytorch to download the pre-trained float32 models<br>from TorchVision, use Pytorch interface to generate int8 quantized models, and<br>implement ILSVRC2012 dataset to test all the models.</p>
<h1 id="Datasets-and-Experimental-Design"><a href="#Datasets-and-Experimental-Design" class="headerlink" title="Datasets and Experimental Design"></a><br>Datasets and Experimental Design</h1><h2 id="Datasets-Data-Sources"><a href="#Datasets-Data-Sources" class="headerlink" title="Datasets/Data Sources"></a>Datasets/Data Sources</h2><p>If a model is efficient, it means that a model not only has acceptable accuracy<br>but also the model size and demand computing power can be small as small<br>possible. Therefore, basic research directions are performance, computing power,<br>and storage.</p>
<p>Indicating a model’s performance needs to have many different indicators. The<br>most basic indicators are Top-1 and Top-5 accuracy. Meanwhile, precision,<br>recall, and F1 score(<a href="#_ENREF_39">Sasaki, 2007</a>) also can evaluate the<br>preference of model abilities in the classification. Finally, the cross-entropy<br>loss can show the inference precision between model and result.</p>
<p>If the models need to evaluate the computing power consumption, when the weight<br>type is determined, the model parameter number can indicate the model calculus<br>consumption and calculate the GFLOPs. However, when the models are quantized,<br>the weight store type is changed, so the test hardware can be determined. The<br>models run quantitative data in the same condition and compare the execution<br>time.</p>
<p>Finally, storage occupancy, memory, and disk usage need to be considered<br>together. Additionally, when the models implement Compute Unified Device<br>Architecture (CUDA)(<a href="#_ENREF_44">Tölke, 2010</a>), the CUDA memory footprint needs<br>to be considered.</p>
<h2 id="Experimental-Design"><a href="#Experimental-Design" class="headerlink" title="Experimental Design"></a>Experimental Design</h2><h3 id="Test-data-collection-and-allocation"><a href="#Test-data-collection-and-allocation" class="headerlink" title="Test data collection and allocation"></a>Test data collection and allocation</h3><p>In chapter 2, ImageNet dataset is chosen when testing the models. So the first<br>step to designing the experiment is to download and deploy ImageNet data. But<br>all the ILSVRC2012 training, valid, and test dataset need to download 158<br>GigaBit(GB). It also needs a very long time to test all the data and the<br>pre-trained models are trained by ILSVRC2012 training set. Therefore, ILSVRC2012<br>valid set which only has 6.4 GB and 50 thousand image files and is testable in<br>ARC3 is chosen in the model benchmark. But when comparing quantized and<br>unquantized models efficiency, CUDA accelerators cannot implement quantized<br>models. So the unquantized model needs to be tested by CPU whose testing time<br>consumption is very high. Therefore, the efficiency testing dataset will use one<br>thousand images that are taken from ILSVRC2012 testing dataset to test the<br>overall running time when implementing all models.</p>
<h3 id="Prepare-trained-pruned-and-quantized-models"><a href="#Prepare-trained-pruned-and-quantized-models" class="headerlink" title="Prepare trained, pruned, and quantized models"></a>Prepare trained, pruned, and quantized models</h3><p>Pre-trained models are downloaded from two different sources. CNN pre-trained<br>models can be downloaded by TorchVision(<a href="#_ENREF_11">Facebook</a>) directly and<br>save their state dictionary. As for pre-pruned models, using the open-source<br>code which was created by the Lin et al.,(<a href="#_ENREF_25">Lin et al., 2020</a>) the<br>pre-pruned models could be downloaded on GitHub and implement.</p>
<p>Now onto Pytorch static quantization guidance. (<a href="#_ENREF_34">Raghuraman<br>Krishnamoorthi</a>) To begin with, Convolution, BN, and ReLU are fused<br>by fuse model function which also needs to be defined by each layer or specific<br>logic. This process fuses the three-layers and quantizes all the parameters<br>together which can prevent the parameters from being independent and decrease<br>the accuracy. Meanwhile, the float32 model needs to be warped by QuantStub and<br>DeQuantStub functions(see Figure 3.1) so that they can convert between float32<br>and int8 tensors. After preparing quantizable models, quantization configuration<br>needs to be defined; containing which type of weights will be converted and what<br>device will use this model. Benefiting from Pytorch structure, using Pytorch API<br>can convert float32 to int8 models and implement them by C++ API. Finally,<br>because quantized models are needed to transfer to C++ API to implement before<br>saving the model, the models need to be serialized.</p>
<blockquote>
<p>  Figure 3.1 quantized model dataflow.</p>
</blockquote>
<h3 id="Test-design"><a href="#Test-design" class="headerlink" title="Test design"></a>Test design</h3><p>Because different model types have different structures that cannot use the same<br>method to detect basic model information. Using<br>TorchSummary(<a href="#_ENREF_5">Chandel</a>) and Pthflops(<a href="#_ENREF_3">Bulat</a>) open-sourced<br>analysis tools, the float32 model parameter number, estimated model size, and<br>GFLOPs are estimated.</p>
<p>In order to compare all types of models horizontally, experiments can be<br>designed to allow all models to complete the same tasks under the same hardware<br>conditions. As the dataset design, 5,000 images that derive from ILSVRC2012<br>testing dataset are input into each model. Besides, the models are all run in<br>one ARC3 standard node(<a href="#_ENREF_9">Dixon, 2016</a>) separately. Its hardware is<br>Intel “Broadwell” 2x E5-2650v4 2.2GHz 24 cores CPU and 128GB of memory which is<br>run on CentOS Linux release 7.6.1810(<a href="#_ENREF_4">CentOS, 2019</a>). Also, all the<br>parameters in the code include batch size and image resizes are the same. These<br>tests are run one hundred times.</p>
<p>To test and compare each float32 pre-trained model performance such as accuracy<br>and precision, all the ImageNet validset is run by all models. So utilizing<br>accelerators like GPU to speed up the tests is very necessary. In this project,<br>ARC4 NVIDIA V100 GPU is used to test these models. However, quantized models<br>cannot use GPU to accelerate the tests. But the quantized models have higher<br>efficiency when using CPU so it does not consume too much time. All the tests<br>are run 20 times. Each test will collect the inference time before the test<br>which time one image classification process.</p>
<h3 id="Data-analysis"><a href="#Data-analysis" class="headerlink" title="Data analysis"></a>Data analysis</h3><p>In order to increase the test accuracy and avoid the noisy data which generate<br>by unpredictable reasons such as electrical support and cluster temperature<br>increase to affect the final results, all the tests are recursively run many<br>times to generate some result sets. All the results are average by Arithmetic<br>Mean(AM) and Geometric Mean(GM). Meanwhile too high or low results are<br>eliminated and getting the 90% confidence interval.</p>
<p>All the collected data will be analysed from vertical and horizontal. Horizontal<br>analysis is mean that all the models will be tested by the same environment.<br>Include the same hardware, dataset, and parameters. Vertical analysis demands<br>the same CNN model is tested by different hardware or compression technologies.<br>This could evaluate the characters from different angles.</p>
<p>Not only the raw data but the analysis data could be saved in the dataset. The<br>entire analysis process and method could be finished by Python scripts and<br>written down into the dataset. This can significantly increase the efficiency of<br>data analysis. Also, the process could be monitored and checked. When the raw<br>data changes or some part of the analysis process needs to be changed, all the<br>analysis results and figures could be regenerated by the scripts.</p>
<p>In conclusion, although the analytical method and process are complex, using<br>Python to analyse and generate visual figures can significantly increase the<br>efficiency in the analysis. Meanwhile, all the processes can be checked and<br>changed by Python scripts; this can elevate the review efficiency. Also, this<br>can increase data accuracy.</p>
<h1 id="Results-of-the-Empirical-Investigation"><a href="#Results-of-the-Empirical-Investigation" class="headerlink" title="Results of the Empirical Investigation"></a><br>Results of the Empirical Investigation</h1><h2 id="Pre-trained-float32-models-comparison"><a href="#Pre-trained-float32-models-comparison" class="headerlink" title="Pre-trained float32 models comparison"></a>Pre-trained float32 models comparison</h2><p>In this test, all the float32 pre-trained models are tested by one V100 GPU<br>accelerator which is installed in Advanced Research Computing4(ARC4) 40core-192G<br>server. Some of the experiments are executed by one ARC3 normal computing<br>24core-128G node.</p>
<p>As Figure 4.1(d) and (e) show, the parameter numbers are positively associated<br>with storage usage. But in the same CNNs architecture, increasing model depth<br>could bring significant GFLOPs increase. (see Figure 4.1(c)) Surprisingly,<br>Figure 4.1(a) shows that the same architecture brings almost the same CUDA<br>memory footprint, the compressed models such as MobileNetv2 and ShuffleNetv2<br>cannot striking decrease the CUDA usage. In this case, ShuffleNetv2 and VGG19_bn<br>consume the lowest and highest resource usage, separately. When using the ARC3<br>CPU computing node to test the models, the model under the VGG architecture has<br>less difference in memory usage. But the models which introduce the residual<br>theory(<a href="#_ENREF_15">He et al., 2016</a>), deeper layer models have a higher memory<br>footprint.</p>
<p>In Figure 4.2(a), when using GPU, along with model layer increase, VGG model<br>inference time is increased but ResNet models inference time is decreased.<br>Compared with uncompressed models, MobileNetv2 and ShuffleNetv2 cannot have very<br>low inference time. ResNet152 achieves the lowest inference time. However,<br>ShuffleNetv2_x0.5 has the lowest resource usage.</p>
<p>Figure 4.2(c) illustrates that when float32 pre-trained models are run by X86<br>CPU, the inference time of the models which introduce residual learning is<br>related to parameter number. But about VGG architecture models, whether using BN<br>or not significantly affects inference time. About no BN VGG models, the<br>inference time also follows the parameter number. However, VGG13_BN has the<br>highest inference time which peaks at 4.2351 seconds.</p>
<p>BN significantly affects running time, but not all the cases. As Figure<br>4.2(d)shows, only Vgg11_BN and VGG13_BN are affected to increase the running<br>time. Especially VGG11_BN, its running time reached at 277.59 seconds. But the<br>highest efficiency model is ShuffleNetv2_0.5 which has a running time of 13.312.</p>
<blockquote>
<p>  Figure 4.1 The CUDA memory footprint after float32 pre-trained models are<br>  loaded (a), the models’ memory footprint when they implement by one ARC3 CPU<br>  computing node(b), the models estimated GFLOPs (c), practical storage usage<br>  (d), and The models’ parameter numbers(e)</p>
</blockquote>
<blockquote>
<p>  Figure 4.2 V100 GPU test all the ImageNet validset, model inference time(a),<br>  and The total time consumption of classifying 5k ImageNet testset full<br>  resolution images(b); Using ARC3 CPU node to test the float32 pre-trained<br>  models, model inference time(c) and The total time consumption of<br>  classifying 5k ImageNet testset full resolution images(d)</p>
</blockquote>
<p>According to Figure 4.3, Benefit from combining state-of-the-art technologies,<br>ResNext101_32x8 owns the highest Top-1 accuracy, Top-5 accuracy, precision,<br>recall, and F1-score. The worst three models are ShuffleNetv2_0.5,<br>ShuffleNetv2_1.0, and ResNet18, Separately. However, compared with ShuffleNetv2,<br>although MobileNetv2 is a compressed model, its performance is between VGG13_bn<br>and VGG16_bn.</p>
<p>To sum up, V100 can bring significant float32 computing power. In this computing<br>environment, different models also can gain significant efficiency. As a result,<br>If the memory is not very strict, choosing relative complex models like<br>ResNet101 model will get achieve better performance and finish the jobs in a<br>short time.</p>
<blockquote>
<p>  Figure 4.3 Each model Top-1 accuracy(a), Top-5 accuracy(b), model precision<br>  and recall(c), and F1-score(d)</p>
</blockquote>
<h2 id="Pruned-models"><a href="#Pruned-models" class="headerlink" title="Pruned models"></a>Pruned models</h2><p>In the previous section, ResNet50 get moderate performance and resource usage.<br>After pruning and fine-tune, the different properties are changed. This also<br>affects efficiency and performance.</p>
<blockquote>
<p>  Figure 4.4 The CUDA memory usage after pre-pruned and fine-tuned models are<br>  loaded (a), the models estimated GFLOPs (b), practical storage usage (c),<br>  and The models’ parameter numbers(d)</p>
</blockquote>
<p>As Figure 4.4 shows, because pruning introduces weight and filter masks to set<br>some weight results to zero, the CUDA memory and storage usage is growing as the<br>network remains approximately 60% or 70% parameters. However, the parameter<br>number and GFLOPs are declining. Because channels and filters are pruned as well<br>when the pruning parameter rate increases to 80%, Both storage usage, and memory<br>footprint go down.</p>
<blockquote>
<p>  Figure 4.5 Compare different ResNet50 pruning rate and original pre-trained<br>  ResNet50 Top-1 accuracy(a), Top-5 accuracy(b), precision, recall(c), and<br>  F1-score(d).</p>
</blockquote>
<p>Through pruning, the model can use a lower parameter number to achieve almost<br>the same as the original model performance. As Figure 4.5(a) shows, although the<br>accuracy continually declines after different rate pruning, the Top-1 accuracy<br>loss only is 1.57% from the original model to pruning 40% parameters. Even if<br>only 20% of parameters remain, the accuracy still stays 69.73%. Figure 4.5 (c)<br>and (d) show that average precision, recall, and F1-score decline tendency is<br>similar to Top-1. About Top-5, the accurate decline is smaller than Top-1. For<br>instance, cutting 80% parameters only decrease 3.706% Top-5 accuracy. (see<br>Figure 4.5(b)) As a result, pruning does not decrease model performance<br>significantly even after cutting most of the parameters.</p>
<blockquote>
<p>  Figure 4.6 Pruning model inference time(a) and classify 5k full resolution<br>  images time(b)</p>
</blockquote>
<p>As illustrated in Figure 4.6(a), when using CUDA to implement the pruned models,<br>pruning virtually does not change ResNet50 inference time too much. In<br>particular, 20% ResNet50 parameter remaining model has the highest inference<br>time which peaks at 0.3388 seconds. Other test results also within the error<br>range.</p>
<p>As for Figure 4.6(b), with pruning rate increase, time consumption elevate.<br>because cutting more parameters need more masks to ignore specific parameters.<br>This needs more running time to run these operations.</p>
<p>In order to find out when the computing power is limited, whether the pruning<br>models have better performance or not. One ARC3 23core-128G Intel(R) Xeon(R) CPU<br>E5-2650 v4 node is chosen to be the testbed. Each ResNet50 model is assigned<br>five thousand ImageNet testset images to test how much time the mode needs to<br>run.</p>
<p>As Figure 4.7 showed, the host memory usage is similar to CUDA.(Figure 4.7(a))<br>As the pruning rate grows, inference time gradually decreases. But the running<br>time does not have this pattern. As Figure 4.7(f) shows, when ResNet50 remains<br>60% parameter, it has the highest running efficiency.</p>
<blockquote>
<p>  Figure 4.7 Using one ARC3 normal CPU node to test the host memory usage(a),<br>  model inference time(c), model running time(e), and their change(b), (d),<br>  and (f).</p>
</blockquote>
<p>To conclude, pruning cannot decrease memory footprint and storage usage when the<br>pruning rate is not very high. If the running platforms have enough float32<br>computing power, pruning cannot have a positive effect to reduce inference and<br>running time. But when the computing power of the devices is limited, it not<br>only can significantly reduce the inference and running time but also can keep<br>the model accuracy at a comparable level.</p>
<h2 id="Quantized-models"><a href="#Quantized-models" class="headerlink" title="Quantized models"></a>Quantized models</h2><p>According to Jacob(<a href="#_ENREF_19">Jacob et al., 2018</a>), statistical quantization<br>can transform parameters to lower precision type. In Pytorch, the parameters are<br>transformed from float32 to int8. In theory, the quantized model memory<br>footprint and storage usage only need a quarter of float32 models. In this test,<br>all the host memory usage, inference, and running time are collected by ARC3 CPU<br>testbed. About storage usage and model performance are also collected by ARC3<br>CPU testbed.</p>
<p>Figure 4.8 shows that except for compressed models such as MobileNetv2 and<br>ShuffleNet, other models can achieve a 75% host memory footprint(see Figure 4.8)<br>and storage usage(see Figure 4.8) reduction close to the theoretical value.<br>Because compressed models have lower basic parameter numbers, the ratio of basic<br>data and parameter storage usage is relatively higher. Compared with VGG and<br>ResNet, this causes the compression rate to be lower.</p>
<blockquote>
<p>  Figure 4.8 upper two figures: quantized models host memory footprint(a) and<br>  compared with float pre-trained models the reduce percentage of host memory<br>  footprint. Lower two figures: quantization models storage space usage(c) and<br>  using storage reduce rate(d).</p>
</blockquote>
<p>Figure 4.9 shows the inference and running time change. Compressed models and<br>lower layers ResNet display the inference time increase when quantizing the<br>models, in particular, ShuffleNetv2_1.0 has more than doubled the inference<br>time. Other models that introduce residual theory(<a href="#_ENREF_15">He et al., 2016</a>)<br>have varying degrees of inference time increase. (see Figure 4.9(a) and (b))<br>Especially, deeper residual block models such as ResNext101_32x8d(<a href="#_ENREF_28">Mahajan et<br>al., 2018</a>) and ResNet101(<a href="#_ENREF_15">He et al., 2016</a>) which have<br>33 bottlenecks. But all the models gained performance improvements in multiple<br>tasks after quantization. Surprisingly, the MobileNetv2, VGG11_bn, and VGG13_bn<br>have nearly doubled their efficiency.</p>
<p>As a result, quantization cannot cause all the models to increase inference time<br>efficiency, using residual theory and deeper models. But it can significantly<br>increase multiple tasks efficiency.</p>
<blockquote>
<p>  Figure 4.9 upper two figures: quantization models inference time(a) and<br>  compared with float32 pre-trained models inference time change(b). lower two<br>  figures: each quantization models running five thousand ImageNet testset<br>  full resolution image time consumption(c) and compared with float32<br>  pre-trained model running time change(d)</p>
</blockquote>
<p>The model performance also could be affected when quantizing the models. As<br>Figure 4.10 shows, all the models have performance loss after quantization.<br>Particularly, compressed models have a significant decrease in all performance<br>indicator. However, traditional models such as VGG and ResNet lose very little<br>performance. The reason is that compressed models have lower parameters. This<br>means that the information of the compressed models already learned is less than<br>higher parameter number models. In order to achieve the same performance as the<br>traditional model, the limited perceptrons and weights needs to learn crucial<br>information. So in compressed models, rate of redundancy and unnecessary<br>perceptrons are very low. When quantizing the compressed models, it means that<br>the rate of crucial information loss is higher than other architecture models.<br>Therefore, compressed models are more affected than other models.</p>
<p>Figure 4.10 and Figure 4.11 also illustrates that MobileNetv2 has the most<br>performance loss including Top-1, Top-5, F1-score, precision, and recall. Its<br>Top-1 accuracy has even dropped below ShuffleNetv2_1.0 and reached 67.272%.<br>Other performance indicators are lower than ShuffleNetv2_1.0. However, Because<br>VGG has a simpler algorithm and structure, it is affected less.</p>
<blockquote>
<p>  Figure 4.10 Compared with float32, quantization models Top-1 accuracy(a) and<br>  change(b); Top-5 accuracy(c) and change(d)</p>
</blockquote>
<blockquote>
<p>  Figure 4.11 Compared with float32, quantized models F1-score(a) and reduce<br>  range(b); Recall/precision(c) and recall(d)/precision(e) change.</p>
</blockquote>
<p>To conclude, although quantized models cannot be used by normal accelerators<br>like GPU, quantization can significantly reduce memory footprint and model<br>storage size. Also, in batch data processing, quantization can significantly<br>increase the model efficiency. However, it could sacrifice model performance and<br>not every model can gain inference efficiency. As a result, if CNNs are used by<br>ARM or X86 CPU architecture devices and not any float32 accelerators, it may be<br>better choice. But quantized transform may change the performance and<br>characteristics significantly. So when choosing quantized models, the trade-off<br>of performance, hardware, and efficiency needs to be considered.</p>
<h2 id="Mixed-comparison-of-pruning-and-quantization"><a href="#Mixed-comparison-of-pruning-and-quantization" class="headerlink" title="Mixed comparison of pruning and quantization"></a>Mixed comparison of pruning and quantization</h2><p>In order to minimize the model hardware demand and find the best efficiency<br>models, in this project, pruning and quantization are compressed ResNet50 to<br>maximize the ResNet50 efficiency. Therefore, different pruning rate models are<br>quantized and evaluate storage usage, time consumption, and performance.</p>
<p>Figure 4.12 shows that the memory footprint and storage usage of the models<br>after the pre-trained ResNet50 is used by quantization or pruning or both. As<br>Figure 4.12(b) and (d) illustrate except ResNet50 which is pruned 30%, 40%, and<br>60%, other models decrease both the memory footprint and storage usage. Although<br>pruning 60% ResNet50 has some storage decline, the extent is very low. As a<br>result, quantization can ensure models to decrease size and memory footprint.<br>But if pruning intends on doing the same thing, it needs to exceed the pruning<br>rate threshold. If quantization and pruning are used in the same model, the<br>decreasing extent of memory and storage usage is very significant</p>
<blockquote>
<p>  Figure 4.12 upper two figures: pre-trained, pruned and quantized ResNet50<br>  models host memory footprint(a) and compared with float32 pre-trained<br>  ResNet50 models the reduced percentage of host memory footprint. Lower two<br>  figures: models storage space usage(c) and compared with float32 pre-trained<br>  ResNet50 models using storage reduce rate(d).</p>
</blockquote>
<p>As Figure 4.13(b) shows that along with the pruning rate increase, the inference<br>time gradually decreases. But the quantization could increase the inference<br>time. As for running time consumption, the highest efficiency model is unpruned<br>and quantized ResNet50. After quantization, the pruning models instead increase<br>the running time. (see Figure 4.13(d))</p>
<blockquote>
<p>  Figure 4.13 upper two figures: pre-trained, pruned, and quantized ResNet50<br>  models inference time(a) and compared with float32 pre-trained ResNet50<br>  inference time change(b). lower two figures: each ResNet50 models running<br>  five thousand ImageNet testset full resolution image time consumption(c) and<br>  compared with float32 pre-trained ResNet50 running time change(d)</p>
</blockquote>
<p>Figure 4.14 and Figure 4.15 illustrates that quantization and pruning affect the<br>model’s preference. But when the quantization type is int8, the preference<br>affection of quantization is smaller than pruning. Meanwhile, the performance<br>loss of pruned quantized models is more related to pruning than quantization<br>int8.</p>
<p>In short, pruning and quantization have a different characteristic which can<br>optimize different properties of models. If using both of them in the same<br>models, it can significantly reduce the model size and memory footprint. But it<br>could reduce the model efficiency and also sacrifice some extent model<br>performance.</p>
<blockquote>
<p>  Figure 4.14 models Top-1 accuracy(a), Top-5 accuracy(c), and their reduction<br>  rate(b)(d)</p>
</blockquote>
<blockquote>
<p>  Figure 4.15 Compared with ResNet50_float32, F1-score(a), Precision/Recall<br>  (c), and their reduction rate(b)(d)(e).</p>
</blockquote>
<h2 id="Horizontal-comparison-of-model-performance-and-efficiency-under-hardware-constraints"><a href="#Horizontal-comparison-of-model-performance-and-efficiency-under-hardware-constraints" class="headerlink" title="Horizontal comparison of model performance and efficiency under hardware constraints"></a>Horizontal comparison of model performance and efficiency under hardware constraints</h2><p>In this section, limited hardware is considered to implement the CNNs. Based on<br>all model performance, time consumption, and resource usage(see <a href="#_A.1__All">Appendix<br>A.1</a> and <a href="#_A.2__All">Appendix A.2</a>) figure out which model has the<br>best specific property and give the recommendation in specific hardware.(see<br>Table 4.1)</p>
<blockquote>
<p>  Table 4.1 the models with the best performance among different properties</p>
</blockquote>
<table>
<thead>
<tr>
<th>model name</th>
<th>device</th>
<th>best property</th>
<th>best property value</th>
<th>comment</th>
</tr>
</thead>
<tbody><tr>
<td>shufflenet_v2_x0_5</td>
<td>cuda</td>
<td>The lowest CUDA memory footprint and running time</td>
<td>CUDAmemory= 924.5328MB running time =  8.639846 seconds</td>
<td>When running model by GPU, it has lowest CUDA memory footprint model, running time, parameter number, and size</td>
</tr>
<tr>
<td>resnext101_32x8d</td>
<td>cuda/ cpu</td>
<td>highest Top-1 , Top-5 accuracy, F1-score, recall, and precision</td>
<td>Top-1= 79.312%,  Top-5= 94.526%,  F1-score=0.7905506, recall=0.79312, precision = 0.7958598</td>
<td>The highest performance CNN model in the experiment</td>
</tr>
<tr>
<td>shufflenet_v2_x0_5</td>
<td>cpu</td>
<td>The lowest host memory footprint, GFLOPs, and parameter number</td>
<td>memory footprint = 1.2510937MB, GFLOPs= 0.05, parameter number= 1.3668 million</td>
<td></td>
</tr>
<tr>
<td>shufflenet_v2_x0_5_ quantization</td>
<td>cpu</td>
<td>The lowest storage usage</td>
<td>1.44MB</td>
<td>using int8 to quantized shufflenet_v2_x0_5 can generate the lowest storage usage model</td>
</tr>
<tr>
<td>resnext101_32x8d_ quantization</td>
<td>cpu</td>
<td>highest Top-1 , Top-5 accuracy, F1-score, recall, and precision in quantized models</td>
<td>Top-1 = 79.072%,  Top-5 = 94.448%,  F1-score = 0.7877295, recall = 0.79042, precision = 0.7932688</td>
<td>The highest performance CNN model in the quantized models</td>
</tr>
<tr>
<td>resnet18</td>
<td>cuda</td>
<td>Inference time</td>
<td>1.1651825 seconds</td>
<td></td>
</tr>
</tbody></table>
<p>In practice, every property needs to be considered to find appropriate models in<br>application development. Imagine a scenario is that the device does not allocate<br>GPU or other accelerators, meanwhile, it does not have very strict accuracy<br>requirements and its available memory and storage are very limited like<br>Industrial Control Motherboard. My recommendation is MobileNetv2_quantization.<br>It could be the best choice because of the low time and resource usage. In the<br>scenario such as video analysis or intelligent photo classification, the<br>applications are run on a personal computer and also do not allocate GPU or open<br>GPU acceleration, I very recommend quantized resnext50_32x4d. Because compared<br>with ResNet50, it has very close inference and running time but itsTop1 and Top5<br>accuracy are nearly 2 and 1 percent higher than that of ResNet50. If the<br>personal computer uses CNN to recognize or separate one specific image objective<br>rather than a batch of jobs, meanwhile the hardware cannot be predicted whether<br>allocate the GPU or not, ResNet18 should be the best model in this scenario.<br>Because it not only has the lowest inference time when using CUDA but also when<br>using CPU, it has comparable inference time. Therefore, different application<br>scenarios need to trade-off different properties of models. Also, hardware<br>limitation is a very important factor in model choice.</p>
<h1 id="Validation-of-Results"><a href="#Validation-of-Results" class="headerlink" title="Validation of Results"></a><br>Validation of Results</h1><h2 id="Testbed"><a href="#Testbed" class="headerlink" title="Testbed"></a>Testbed</h2><p>The testbed is one of the most important factors in experiments. Because valid<br>results are based on a stable test environment, it can ensure the results are<br>more valuable. Meanwhile, stable tested can prevent external cases that could<br>affect machine statuses such as unstable electricity and temperature. Also, the<br>experiment needs time, ensuring each experiment item is in the same environment,<br>which is equally important.</p>
<p>In order to ensure the tests are not interfered with. University of Leeds ARC3<br>and ARC4 are chosen to generate and test all models. They have a professional<br>monitor, power sustainment, and maintenance. Although one node only can be used<br>48 hours at a time and applying a single core is easier in the multi-user<br>operating system(OS), all the tests only introduce one whole individual node<br>when needed to use CPU. In the test process, ARC4 computing nodes are more<br>stable and efficient to finish the jobs. But it needs to queue a very long time<br>to apply one node. (see Figure 5.1) Therefore, most of the CPU tests are<br>finished by ARC3. But each test is noted the node information in order to ensure<br>every individual test item is tested by the same type of node, dataset, and<br>testsets. (see Figure 5.2) So the comparisons between the models are fair and<br>equal. In practice, more advanced devices and technologies are used more<br>frequently. So in the GPU tests, state-of-the-art V100 GPU is introduced in the<br>float32 models testing. This can make the result more valuable.</p>
<p>![](C:/Users/bawanag/Desktop/Comparison-of-Different-Deep-Learning-technologies /ecc072051490b07f5d629eb15115448d.png)</p>
<blockquote>
<p>  Figure 5.1 ARC4 Sun Grid Engine (SGE) queue</p>
</blockquote>
<p>![](C:/Users/bawanag/Desktop/Comparison-of-Different-Deep-Learning-technologies /360ec5d40ad1e821aee5eca16d2301f3.png)</p>
<blockquote>
<p>  Figure 5.2 Log testbed information in each test.</p>
</blockquote>
<h2 id="Data-collection-and-analysis"><a href="#Data-collection-and-analysis" class="headerlink" title="Data collection and analysis"></a>Data collection and analysis</h2><p>In experiments, unpredictable interruption and data fluctuation are inevitable.<br>So repeating experiments and scientific summarization can minimize their effect.</p>
<p>Regarding dataset selection, because all the models are pre-trained by<br>ILSVRC2012 dataset which includes one thousand classifications. If the testset<br>cannot cover all the ImageNet 1000 classes or each class only has one or two<br>images to be tested. The results of model performance are not accurate.<br>Therefore, in this project, all the ILSVRC2012 validset is used to test model<br>performance even though it needs more time in every test epoch. This produces<br>test results that are valuable and closer to another more professional test.</p>
<p>About data analysis, eliminating too high or low recorded values is one of the<br>very important methods to avoid perturbation. Therefore, every individual<br>experiment is run dozens of times recursively. Because of available hardware<br>limitations, test model performance to collect such Top-1 accuracy, Top-5<br>accuracy, precision, recall, and F1-score is run and collected 20 times. But<br>comparing model efficiency between models which only need to finish the same<br>jobs in the same environment, an experiment is run 100 times. All the recursive<br>collection data is averaged by geometric or arithmetic mean and given the 90%<br>confidence interval. (<a href="#_ENREF_40">Seixas et al., 1988</a>) This can reduce<br>affection from data perturbation.</p>
<p>Python was selected as a tool for data analysis. Because compared with manual<br>calculation or automatic tools like Excel, Python can trace the data flow,<br>generating algorithms, calculational logic, and plot figures logic easier. Also,<br>when the experiment data or analysis method are changed, all the analysis<br>parameters and algorithms are modified easier. This can significantly reduce the<br>data analysis works and increase the efficiency in reviewing all the analysis<br>processes and ensuring the data is correct.</p>
<h2 id="Compare-results-with-other-same-researches"><a href="#Compare-results-with-other-same-researches" class="headerlink" title="Compare results with other same researches"></a>Compare results with other same researches</h2><p>This section focuses on comparing the results corrected by this project and<br>other same researches to demonstrate that the results are valid.</p>
<p>As Table 5.1 shows that the comparison of performance evaluation of the<br>pre-trained model between this project and Pytorch Model<br>Zoo(<a href="#_ENREF_11">Facebook</a>), the results are very close between two different<br>research experiments. The only major difference is the exact number after the<br>decimal point.</p>
<blockquote>
<p>  Table 5.1 Compared the model performance results between this project and<br>  Pytorch Model zoo(<a href="#_ENREF_11">Facebook</a>).</p>
</blockquote>
<table>
<thead>
<tr>
<th>model name</th>
<th>Top-1 accuracy(this project unit: %)</th>
<th>Top-1 accuracy(Pytorch Hub unit: %)</th>
<th>Top-5 accuracy(this project unit: %)</th>
<th>Top-5 accuracy(Pytorch Hub unit: %)</th>
</tr>
</thead>
<tbody><tr>
<td>shufflenet_v2_x1_0</td>
<td>69.362</td>
<td>69.36</td>
<td>88.316</td>
<td>88.32</td>
</tr>
<tr>
<td>mobilenetv2</td>
<td>71.878</td>
<td>71.88</td>
<td>90.286</td>
<td>90.29</td>
</tr>
<tr>
<td>resnet18</td>
<td>69.758</td>
<td>69.76</td>
<td>89.076</td>
<td>89.08</td>
</tr>
<tr>
<td>resnet34</td>
<td>73.314</td>
<td>73.3</td>
<td>91.42</td>
<td>91.42</td>
</tr>
<tr>
<td>resnet50</td>
<td>76.13</td>
<td>76.15</td>
<td>92.862</td>
<td>92.87</td>
</tr>
<tr>
<td>resnet101</td>
<td>77.374</td>
<td>77.37</td>
<td>93.546</td>
<td>93.56</td>
</tr>
<tr>
<td>resnet152</td>
<td>78.312</td>
<td>78.31</td>
<td>94.046</td>
<td>94.06</td>
</tr>
<tr>
<td>vgg11</td>
<td>69.02</td>
<td>69.02</td>
<td>88.628</td>
<td>88.63</td>
</tr>
<tr>
<td>vgg11_bn</td>
<td>70.37</td>
<td>73.3</td>
<td>89.81</td>
<td>91.42</td>
</tr>
<tr>
<td>vgg13</td>
<td>69.93</td>
<td>69.93</td>
<td>89.246</td>
<td>89.25</td>
</tr>
<tr>
<td>vgg13_bn</td>
<td>71.586</td>
<td>71.55</td>
<td>90.374</td>
<td>90.37</td>
</tr>
<tr>
<td>vgg16</td>
<td>71.592</td>
<td>71.59</td>
<td>90.382</td>
<td>90.38</td>
</tr>
<tr>
<td>vgg16_bn</td>
<td>73.36</td>
<td>73.37</td>
<td>91.516</td>
<td>91.5</td>
</tr>
<tr>
<td>vgg19</td>
<td>72.376</td>
<td>72.38</td>
<td>90.876</td>
<td>90.88</td>
</tr>
<tr>
<td>vgg19_bn</td>
<td>74.218</td>
<td>74.24</td>
<td>91.842</td>
<td>91.85</td>
</tr>
<tr>
<td>resnext50_32x4d</td>
<td>77.618</td>
<td>77.62</td>
<td>93.698</td>
<td>93.7</td>
</tr>
<tr>
<td>resnext101_32x8d</td>
<td>79.312</td>
<td>79.31</td>
<td>94.526</td>
<td>94.53</td>
</tr>
</tbody></table>
<p>Table 5.2 shows that the parameter number, GFLOPs, Top-1, and Top-5 accuracy<br>between tested by this project and the research of Lin et al. (<a href="#_ENREF_25">Lin et al.,<br>2020</a>) Although the accuracy evaluation has certain deviations,<br>these deviations are within the error ranges. The reason is that the test<br>dataset may be different.</p>
<blockquote>
<p>  Table 5.2 Compared the model performance results between this project and<br>  research from Li et al. (<a href="#_ENREF_25">Lin et al., 2020</a>)</p>
</blockquote>
<table>
<thead>
<tr>
<th>model name</th>
<th>parameter number(this project unit: million)</th>
<th>parameter number(Li et al. unit: million)</th>
<th>GFLOPs(this project)</th>
<th>GFLOPs(Li et al.)</th>
<th>Top-1 accuracy(this project %)</th>
<th>Top-1 accuracy(Li et al. unit:%)</th>
<th>Top-5 accuracy(this project %)</th>
<th>Top-5 accuracy(Li et al. unit:%)</th>
</tr>
</thead>
<tbody><tr>
<td>pruning_resnet50_0.2_remain</td>
<td>7.1777</td>
<td>7.18</td>
<td>0.94</td>
<td>0.93</td>
<td>69.378</td>
<td>69.43</td>
<td>89.156</td>
<td>89.23</td>
</tr>
<tr>
<td>pruning_resnet50_0.4_remain</td>
<td>10.3957</td>
<td>10.4</td>
<td>1.51</td>
<td>1.51</td>
<td>72.912</td>
<td>73.04</td>
<td>91.112</td>
<td>91.18</td>
</tr>
<tr>
<td>pruning_resnet50_0.6_remain</td>
<td>14.5328</td>
<td>14.53</td>
<td>2.23</td>
<td>2.23</td>
<td>74.58</td>
<td>74.68</td>
<td>92.174</td>
<td>92.17</td>
</tr>
<tr>
<td>pruning_resnet50_0.7_remain</td>
<td>16.9452</td>
<td>16.95</td>
<td>2.65</td>
<td>2.64</td>
<td>75.204</td>
<td>75.22</td>
<td>92.396</td>
<td>92.41</td>
</tr>
</tbody></table>
<p>Figure 5.3(b) illustrates the memory consumption of VGG16 in the research of<br>Muhammed et al. (<a href="#_ENREF_29">Muhammed et al., 2017</a>) is 3.03% less than VGG19.<br>These results are almost same as the Figure 5.3(a) which is 3.6%. Also, the<br>ResNet101 is less 27.6% than ResNet152 in (b). This result also is close to<br>30.5% in (a).</p>
<blockquote>
<p>  Figure 5.3 Compared the memory footprint statistic between this project(a)<br>  and Muhammed et al. (<a href="#_ENREF_29">Muhammed et al., 2017</a>)</p>
</blockquote>
<p>In Figure 5.4(b), Qin et al. finds that after pruning and quantization, the<br>inference time of ResNet50 increase to approximate 20%. Its result is very close<br>to the result of quantized ResNet50_0.2_remain in this project which increases<br>by 24%.</p>
<blockquote>
<p>  Figure 5.4 ResNet50 and quantized pruned ResNet50 inference time(a),<br>  inference time change(b), and quantization + pruning VGG16/ResNet50<br>  inference time change(c) from the research of Qin et al. (<a href="#_ENREF_33">Qin et al.,<br>  2018</a>)</p>
</blockquote>
<p>In conclusion, different statistic result samples are very close to previous<br>research results. So the results which are collected in this project should be<br>valid.</p>
<h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In order to maximally increase the accuracy of data. I utilize ARC testbed and<br>test the models dozens of time to collect raw testing results. In analysis,<br>setting confidence interval and using different average algorithm also can avoid<br>the unusual data to disturb the final results. Finally, after testing, comparing<br>the results between this project and other researches also can help me to fix<br>the errors in test or analysis.</p>
<h1 id="Conclusions-and-Future-Work"><a href="#Conclusions-and-Future-Work" class="headerlink" title="Conclusions and Future Work"></a><br>Conclusions and Future Work</h1><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>These projects evaluate different CNN architectures which base off the original<br>design or are the process by quantization and pruning. The conclusions of<br>performance and efficiency for CNN and compressed technology are representation<br>are as follows:</p>
<ol>
<li><p>Under the same model architecture, a shallower network depth can be designed<br> to achieve a balance between model efficiency and performance.</p>
</li>
<li><p>Pruning can significantly decrease inference and running time. But when the<br> pruning rate is not high enough, the memory footprint and storage usage<br> could increase.</p>
</li>
<li><p>Quantization can significantly reduce the model size, memory footprint, and<br> running time. But only the models which introduce residual theory(<a href="#_ENREF_15">He et<br> al., 2016</a>) can keep or reduce the inference time when the<br> quantized type is int8.</p>
</li>
<li><p>Mixing different compression technologies such as quantizing compressed<br> models or quantizing pruned models may not bring the overall model energy<br> efficiency gains.</p>
</li>
<li><p>A model that pursues the ultimate performance of a certain attribute usually<br> sacrifices other attributes significantly. Therefore, when choosing a model,<br> it is necessary to weigh every Key Performance Parameter.</p>
</li>
</ol>
<p>However, there are some flaws and deficiencies in this project. To begin with,<br>because of the project time limit and ARC queue waiting time, the models cannot<br>be tested by different GPUs and CPUs. Moreover, horizontal tests of quantized<br>models should use ARM devices which are used quantized models more generally in<br>practice. Meanwhile, the testbed is based on KVM virtual machine. It may have a<br>certain performance reduction and be affected by other processes from other<br>users. So these tests should be based on independent equipment. This can<br>minimize external interference. Plus, because of hardware and technical<br>limitations, pruning cannot generate and test different model architecture.<br>Meanwhile, quantization only can transform the parameter type from float32 to<br>int8. Also, the dynamic quantization and quantization fine-tune cannot be<br>implemented and tested in their models. Finally, some hypothesize such residual<br>theory could affect the inference time of quantized models that have not been<br>proved.</p>
<h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>This project has some flaws and content as the previous section discusses which<br>can be fixed and added it later. In the beginning, various individual hardware<br>and limited computing power devices should include testbeds in order to test the<br>efficiency of the models in various hardware environments. This includes using<br>different technologies to compress different architecture models and testing<br>them. Finally, the log collection should be more detailed. For example, the time<br>recording should be accurate for each layer or operation. This can help to prove<br>the hypothesis of research and give detailed guidance when using compression<br>technologies.</p>
<p>List of References</p>
<blockquote>
<p>  ABADI, M., AGARWAL, A., BARHAM, P., BREVDO, E., CHEN, Z., CITRO, C.,<br>  CORRADO, G. S., DAVIS, A., DEAN, J. &amp; DEVIN, M. 2016. Tensorflow:<br>  Large-scale machine learning on heterogeneous distributed systems. <em>arXiv<br>  preprint arXiv:1603.04467</em>.</p>
</blockquote>
<blockquote>
<p>  ABADI, M., ISARD, M. &amp; MURRAY, D. G. A computational model for TensorFlow:<br>  an introduction. Proceedings of the 1st ACM SIGPLAN International Workshop<br>  on Machine Learning and Programming Languages, 2017. 1-7.</p>
</blockquote>
<blockquote>
<p>  BULAT, A. <em>pytorch-estimate-flops</em> [Online]. Available:<br>  <a target="_blank" rel="noopener" href="https://github.com/1adrianb/pytorch-estimate-flops">https://github.com/1adrianb/pytorch-estimate-flops</a> [Accessed].</p>
</blockquote>
<blockquote>
<p>  CENTOS. 2019. <em>CentOS-7 (1810) Release Notes</em> [Online]. Available:<br>  <a target="_blank" rel="noopener" href="https://wiki.centos.org/Manuals/ReleaseNotes/CentOS7.1810">https://wiki.centos.org/Manuals/ReleaseNotes/CentOS7.1810</a> [Accessed].</p>
</blockquote>
<blockquote>
<p>  CHANDEL, S. <em>Keras style model.summary() in PyTorch</em> [Online]. Available:<br>  <a target="_blank" rel="noopener" href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a> [Accessed].</p>
</blockquote>
<blockquote>
<p>  CONTRIBUTORS, P. PyTorch online documentation.</p>
</blockquote>
<blockquote>
<p>  DENIL, M., SHAKIBI, B., DINH, L., RANZATO, M. A. &amp; DE FREITAS, N. Predicting<br>  parameters in deep learning. Advances in neural information processing<br>  systems, 2013. 2148-2156.</p>
</blockquote>
<blockquote>
<p>  DENTON, E. L., ZAREMBA, W., BRUNA, J., LECUN, Y. &amp; FERGUS, R. Exploiting<br>  linear structure within convolutional networks for efficient evaluation.<br>  Advances in neural information processing systems, 2014. 1269-1277.</p>
</blockquote>
<blockquote>
<p>  DIXON, M. 2016. <em>ARC3</em> [Online]. Available:<br>  <a target="_blank" rel="noopener" href="https://arc.leeds.ac.uk/wp-content/uploads/2016/07/ARC3-Service.pdf">https://arc.leeds.ac.uk/wp-content/uploads/2016/07/ARC3-Service.pdf</a><br>  [Accessed].</p>
</blockquote>
<blockquote>
<p>  DUNDAR, A., JIN, J., GOKHALE, V., KRISHNAMURTHY, B., CANZIANI, A., MARTINI,<br>  B. &amp; CULURCIELLO, E. Accelerating deep neural networks on mobile processor<br>  with embedded programmable logic. Neural information processing systems<br>  conference (NIPS), 2013.</p>
</blockquote>
<blockquote>
<p>  FACEBOOK. <em>PYTORCH HUB</em> [Online]. Available: <a target="_blank" rel="noopener" href="https://pytorch.org/hub/">https://pytorch.org/hub/</a><br>  [Accessed].</p>
</blockquote>
<blockquote>
<p>  FRANKLE, J. &amp; CARBIN, M. 2018. The lottery ticket hypothesis: Finding<br>  sparse, trainable neural networks. <em>arXiv preprint arXiv:1803.03635</em>.</p>
</blockquote>
<blockquote>
<p>  GONG, Y., LIU, L., YANG, M. &amp; BOURDEV, L. 2014. Compressing deep<br>  convolutional networks using vector quantization. <em>arXiv preprint<br>  arXiv:1412.6115</em>.</p>
</blockquote>
<blockquote>
<p>  HAEFFELE, B. D. &amp; VIDAL, R. Global optimality in neural network training.<br>  Proceedings of the IEEE Conference on Computer Vision and Pattern<br>  Recognition, 2017. 7331-7339.</p>
</blockquote>
<blockquote>
<p>  HE, K., ZHANG, X., REN, S. &amp; SUN, J. Deep residual learning for image<br>  recognition. Proceedings of the IEEE conference on computer vision and<br>  pattern recognition, 2016. 770-778.</p>
</blockquote>
<blockquote>
<p>  HOWARD, A. G., ZHU, M., CHEN, B., KALENICHENKO, D., WANG, W., WEYAND, T.,<br>  ANDREETTO, M. &amp; ADAM, H. 2017. Mobilenets: Efficient convolutional neural<br>  networks for mobile vision applications. <em>arXiv preprint arXiv:1704.04861</em>.</p>
</blockquote>
<blockquote>
<p>  HU, H., PENG, R., TAI, Y.-W. &amp; TANG, C.-K. 2016. Network trimming: A<br>  data-driven neuron pruning approach towards efficient deep architectures.<br>  <em>arXiv preprint arXiv:1607.03250</em>.</p>
</blockquote>
<blockquote>
<p>  HUBEL, D. H. &amp; WIESEL, T. N. 1968. Receptive fields and functional<br>  architecture of monkey striate cortex. <em>The Journal of physiology,</em> 195**,**<br>  215-243.</p>
</blockquote>
<blockquote>
<p>  JACOB, B., KLIGYS, S., CHEN, B., ZHU, M., TANG, M., HOWARD, A., ADAM, H. &amp;<br>  KALENICHENKO, D. Quantization and training of neural networks for efficient<br>  integer-arithmetic-only inference. Proceedings of the IEEE Conference on<br>  Computer Vision and Pattern Recognition, 2018. 2704-2713.</p>
</blockquote>
<blockquote>
<p>  JAIN, A., BHATTACHARYA, S., MASUDA, M., SHARMA, V. &amp; WANG, Y. 2020.<br>  Efficient Execution of Quantized Deep Learning Models: A Compiler Approach.<br>  <em>arXiv preprint arXiv:2006.10226</em>.</p>
</blockquote>
<blockquote>
<p>  KRIZHEVSKY, A. 2014. One weird trick for parallelizing convolutional neural<br>  networks. <em>arXiv preprint arXiv:1404.5997</em>.</p>
</blockquote>
<blockquote>
<p>  KRIZHEVSKY, A. &amp; HINTON, G. 2009. Learning multiple layers of features from<br>  tiny images.</p>
</blockquote>
<blockquote>
<p>  LECUN, Y., BOTTOU, L., BENGIO, Y. &amp; HAFFNER, P. 1998. Gradient-based<br>  learning applied to document recognition. <em>Proceedings of the IEEE,</em> 86**,**<br>  2278-2324.</p>
</blockquote>
<blockquote>
<p>  LI, H., KADAV, A., DURDANOVIC, I., SAMET, H. &amp; GRAF, H. P. 2016. Pruning<br>  filters for efficient convnets. <em>arXiv preprint arXiv:1608.08710</em>.</p>
</blockquote>
<blockquote>
<p>  LIN, M., JI, R., LI, S., YE, Q., TIAN, Y., LIU, J. &amp; TIAN, Q. 2020. Filter<br>  sketch for network pruning. <em>arXiv preprint arXiv:2001.08514</em>.</p>
</blockquote>
<blockquote>
<p>  LIU, Z., LI, J., SHEN, Z., HUANG, G., YAN, S. &amp; ZHANG, C. Learning efficient<br>  convolutional networks through network slimming. Proceedings of the IEEE<br>  International Conference on Computer Vision, 2017. 2736-2744.</p>
</blockquote>
<blockquote>
<p>  MA, N., ZHANG, X., ZHENG, H.-T. &amp; SUN, J. Shufflenet v2: Practical<br>  guidelines for efficient cnn architecture design. Proceedings of the<br>  European conference on computer vision (ECCV), 2018. 116-131.</p>
</blockquote>
<blockquote>
<p>  MAHAJAN, D., GIRSHICK, R., RAMANATHAN, V., HE, K., PALURI, M., LI, Y.,<br>  BHARAMBE, A. &amp; VAN DER MAATEN, L. Exploring the limits of weakly supervised<br>  pretraining. Proceedings of the European Conference on Computer Vision<br>  (ECCV), 2018. 181-196.</p>
</blockquote>
<blockquote>
<p>  MUHAMMED, M. A. E., AHMED, A. A. &amp; KHALID, T. A. Benchmark analysis of<br>  popular imagenet classification deep cnn architectures. 2017 International<br>  Conference On Smart Technologies For Smart Nation (SmartTechCon), 2017.<br>  IEEE, 902-907.</p>
</blockquote>
<blockquote>
<p>  NARASIMHAN, S. Nvidia clocks world’s fastest bert training time and largest<br>  transformer based model, paving path for advanced conversational ai.<br>  Technical report, 2019. URL <a target="_blank" rel="noopener" href="https://devblogs/">https://devblogs</a>. nvidia.<br>  com/training-bert-with ….</p>
</blockquote>
<blockquote>
<p>  PASZKE, A., GROSS, S., CHINTALA, S., CHANAN, G., YANG, E., DEVITO, Z., LIN,<br>  Z., DESMAISON, A., ANTIGA, L. &amp; LERER, A. 2017. Automatic differentiation in<br>  pytorch.</p>
</blockquote>
<blockquote>
<p>  PASZKE, A., GROSS, S., MASSA, F., LERER, A., BRADBURY, J., CHANAN, G.,<br>  KILLEEN, T., LIN, Z., GIMELSHEIN, N. &amp; ANTIGA, L. Pytorch: An imperative<br>  style, high-performance deep learning library. Advances in neural<br>  information processing systems, 2019. 8026-8037.</p>
</blockquote>
<blockquote>
<p>  QIN, Q., REN, J., YU, J., WANG, H., GAO, L., ZHENG, J., FENG, Y., FANG, J. &amp;<br>  WANG, Z. To compress, or not to compress: Characterizing deep learning model<br>  compression for embedded inference. 2018 IEEE Intl Conf on Parallel &amp;<br>  Distributed Processing with Applications, Ubiquitous Computing &amp;<br>  Communications, Big Data &amp; Cloud Computing, Social Computing &amp; Networking,<br>  Sustainable Computing &amp; Communications<br>  (ISPA/IUCC/BDCloud/SocialCom/SustainCom), 2018. IEEE, 729-736.</p>
</blockquote>
<blockquote>
<p>  RAGHURAMAN KRISHNAMOORTHI, S. W. <em>STATIC QUANTIZATION WITH EAGER MODE IN<br>  PYTORCH</em> [Online]. Available:<br>  <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#beta-static-quantization-with-eager-mode-in-pytorch">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#beta-static-quantization-with-eager-mode-in-pytorch</a><br>  [Accessed].</p>
</blockquote>
<blockquote>
<p>  REUTHER, A., MICHALEAS, P., JONES, M., GADEPALLY, V., SAMSI, S. &amp; KEPNER, J.</p>
<ol start="2019">
<li>Survey and benchmarking of machine learning accelerators. <em>arXiv<br>preprint arXiv:1908.11348</em>.</li>
</ol>
</blockquote>
<blockquote>
<p>  RUSSAKOVSKY, O., DENG, J., SU, H., KRAUSE, J., SATHEESH, S., MA, S., HUANG,<br>  Z., KARPATHY, A., KHOSLA, A. &amp; BERNSTEIN, M. 2015. Imagenet large scale<br>  visual recognition challenge. <em>International journal of computer vision,</em><br>  115**,** 211-252.</p>
</blockquote>
<blockquote>
<p>  SANDLER, M., HOWARD, A., ZHU, M., ZHMOGINOV, A. &amp; CHEN, L.-C. Mobilenetv2:<br>  Inverted residuals and linear bottlenecks. Proceedings of the IEEE<br>  conference on computer vision and pattern recognition, 2018. 4510-4520.</p>
</blockquote>
<blockquote>
<p>  SANH, V., DEBUT, L., CHAUMOND, J. &amp; WOLF, T. 2019. DistilBERT, a distilled<br>  version of BERT: smaller, faster, cheaper and lighter. <em>arXiv preprint<br>  arXiv:1910.01108</em>.</p>
</blockquote>
<blockquote>
<p>  SASAKI, Y. 2007. The truth of the f-measure. 2007.</p>
</blockquote>
<blockquote>
<p>  SEIXAS, N. S., ROBINS, T. G. &amp; MOULTON, L. H. 1988. The use of geometric and<br>  arithmetic mean exposures in occupational epidemiology. <em>American journal of<br>  industrial medicine,</em> 14**,** 465-477.</p>
</blockquote>
<blockquote>
<p>  SIMONYAN, K. &amp; ZISSERMAN, A. 2014. Very deep convolutional networks for<br>  large-scale image recognition. <em>arXiv preprint arXiv:1409.1556</em>.</p>
</blockquote>
<blockquote>
<p>  SRINIVAS, S. &amp; BABU, R. V. 2015. Data-free parameter pruning for deep neural<br>  networks. <em>arXiv preprint arXiv:1507.06149</em>.</p>
</blockquote>
<blockquote>
<p>  SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S., ANGUELOV, D., ERHAN,<br>  D., VANHOUCKE, V. &amp; RABINOVICH, A. Going deeper with convolutions.<br>  Proceedings of the IEEE conference on computer vision and pattern<br>  recognition, 2015. 1-9.</p>
</blockquote>
<blockquote>
<p>  TöLKE, J. 2010. Implementation of a Lattice Boltzmann kernel using the<br>  Compute Unified Device Architecture developed by nVIDIA. <em>Computing and<br>  Visualization in Science,</em> 13**,** 29.</p>
</blockquote>
<blockquote>
<p>  XIE, S., GIRSHICK, R., DOLLáR, P., TU, Z. &amp; HE, K. Aggregated residual<br>  transformations for deep neural networks. Proceedings of the IEEE conference<br>  on computer vision and pattern recognition, 2017. 1492-1500.</p>
</blockquote>
<blockquote>
<p>  YANG, Z., SHOU, L., GONG, M., LIN, W. &amp; JIANG, D. Model compression with<br>  two-stage multi-teacher knowledge distillation for web question answering<br>  system. Proceedings of the 13th International Conference on Web Search and<br>  Data Mining, 2020. 690-698.</p>
</blockquote>
<blockquote>
<p>  ZHANG, X., ZHOU, X., LIN, M. &amp; SUN, J. Shufflenet: An extremely efficient<br>  convolutional neural network for mobile devices. Proceedings of the IEEE<br>  conference on computer vision and pattern recognition, 2018. 6848-6856.</p>
</blockquote>
<p>Appendix A<br>External Materials</p>
<h2 id="A-1-All-model-performance"><a href="#A-1-All-model-performance" class="headerlink" title="A.1 All model performance"></a>A.1 All model performance</h2><p>![](C:/Users/bawanag/Desktop/Comparison-of-Different-Deep-Learning-technologies /b8995f4116a193f21b2962211b44d11b.emf)</p>
<h2 id="A-2-All-model-resource-and-time-consumption"><a href="#A-2-All-model-resource-and-time-consumption" class="headerlink" title="A.2 All model resource and time consumption"></a>A.2 All model resource and time consumption</h2><p>![](C:/Users/bawanag/Desktop/Comparison-of-Different-Deep-Learning-technologies /eb0a610665be3c8c8ddb6c343f14cc87.emf)</p>
<h2 id="A-3-Project-Git-repository-link"><a href="#A-3-Project-Git-repository-link" class="headerlink" title="A.3 Project Git repository link"></a>A.3 Project Git repository link</h2><p>Main Gitea link:</p>
<p><a target="_blank" rel="noopener" href="http://gitea2.taoyuanjian.fun:57281/bawanag/compare_models_and_compressed_tech.git">http://gitea2.taoyuanjian.fun:57281/bawanag/compare_models_and_compressed_tech.git</a></p>
<p>Backup link:</p>
<p><a target="_blank" rel="noopener" href="http://gitea.taoyuanjian.fun:57281/bawanag/choose_your_model_in_EDLS.git">http://gitea.taoyuanjian.fun:57281/bawanag/choose_your_model_in_EDLS.git</a></p>
<p>Appendix B<br>Ethical Issues Addressed</p>
<p>This empirical investigation project is void of any ethical issue</p>
</div></div><div class="nextinfo"><p>上一篇：没有了</p><p>下一篇：<a href="/2020/06/25/%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95%E7%9A%84%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88Muti-Perceptron%EF%BC%89%E9%80%9A%E8%BF%87%E8%8A%B1%E7%93%A3%E4%BB%A5%E5%8F%8A%E8%8A%B1%E8%90%BC%E7%9A%84%E9%95%BF%E5%AE%BD%E5%8C%BA%E5%88%86%E5%8F%98%E8%89%B2%E9%B8%A2%E5%B0%BE%EF%BC%8C%E5%B1%B1%E9%B8%A2%E5%B0%BE%EF%BC%8C%E4%BB%A5%E5%8F%8A%E7%BB%B4%E5%90%89%E5%B0%BC%E4%BA%9A%E9%B8%A2%E5%B0%BE/">使用简单的多层感知器（Muti-Perceptron）通过花瓣以及花萼的长宽区分变色鸢尾，山鸢尾，以及维吉尼亚鸢尾</a></p></div><div class="news_pl"><div id="comment_container"></div>
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<script src="/js/md5.min.js"></script>
<script>const gitalk = new Gitalk({
    clientID: '5975a9fb9158db053cc5',
    clientSecret: '4121435bbf066fbfee744fd3abf8cb384e94fdae',
    repo: 'git@github.com:bawanag/bawanag.github.io.git',
    owner: 'bawanag',
    admin: ['bawanag'],
    id: md5("2020/12/20/Comparison-of-Different-Deep-Learning-technologies/"),      // Ensure uniqueness and length less than 50
    distractionFreeMode: true  // Facebook-like distraction free mode
})
gitalk.render('comment_container')</script></div></div></div></main><aside class="r_box"><div class="card box"><h2>我的名片</h2><div class="box_con"><p>Bawanag</p>
<p>职业：AI炼丹师、全栈软件工程师、Java、算法劝退师</p>
<p>Email：tyj8631775@outlook.com</p>
</div></div><div class="category box"><h2>文章分类</h2><div class="box_con"></div></div></aside></article><footer><section class="footer_bottom"><div class="footer_container"><p class="copyright"></p><ul class="social_network"></ul></div></section></footer><div class="cd-top"><i class="iconfont icon-top"></i></div>
<script src="/js/jquery.min.js"></script>
<script src="/js/scrollreveal.js"></script>
<script src="/js/hc-sticky.js"></script>
<script src="/js/canvas-nest.js" type="text/javascript" color="47,135,193" opacity="0.7" zIndex="-2" count="199"></script><script src="https://cdn.bootcss.com/highlight.js/9.15.9/highlight.min.js"></script><script src="https://cdn.bootcss.com/highlight.js/9.15.9/languages/java.min.js"></script><script src="https://cdn.bootcss.com/highlight.js/9.15.9/languages/javascript.min.js"></script><script type="text/javascript">hljs.initHighlightingOnLoad();</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="/js/common.js?v=202101252249.js"></script>
<script src="/js/index.js?v=202101252249.js"></script>
</body></html>